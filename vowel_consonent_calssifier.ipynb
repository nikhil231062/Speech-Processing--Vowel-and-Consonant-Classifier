{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='D:/codes/M.Tech_proj/SPP_assignment_data/SPP_assignment_data/clean_speech/clean_speech/test'\n",
    "a=[]\n",
    "i=[]\n",
    "u=[]\n",
    "for each in os.listdir(path):\n",
    "    wav,sr=librosa.load(path+'/'+each)\n",
    "    f0,_,_=librosa.pyin(wav,fmin=librosa.note_to_hz('C2'),fmax=librosa.note_to_hz('C7'),sr=sr)\n",
    "    f0[np.isnan(f0)]=0\n",
    "    # lpc=librosa.lpc(wav,order=16)\n",
    "    # mfcc=librosa.feature.mfcc(wav)\n",
    "    # spect=librosa.stft(wav)\n",
    "    # spect=librosa.amplitude_to_db(np.abs(spect),ref=np.max)\n",
    "    if each.split('_')[1]=='AA':\n",
    "        a.append([f0,0])\n",
    "    if each.split('_')[1]=='II':\n",
    "        i.append([f0,1])\n",
    "    if each.split('_')[1]=='UU':\n",
    "        u.append([f0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=a+i+u\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp=open('test_clean','wb')\n",
    "pickle.dump(dataset,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model(nn.Module):\n",
    "    def __init__(self,input,hidden,out):\n",
    "        super(lstm_model,self).__init__()\n",
    "        self.l1=nn.LSTM(input,hidden,dtype=torch.double)\n",
    "        self.l2=nn.LSTM(hidden,hidden,dtype=torch.double)\n",
    "        self.l3=nn.Linear(hidden,out,dtype=torch.double)\n",
    "        self.l4=nn.Softmax(dim=2)\n",
    "    def forward(self,data):\n",
    "        h0,_=self.l1(data)\n",
    "        h0,_=self.l2(h0)\n",
    "        e=self.l3(h0)\n",
    "        return self.l4(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp=open('D:/codes/M.Tech_proj/train_clean_mfcc','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 87)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 642.1609072100006\n",
      "loss after 1 epoch is 641.7084505796892\n",
      "loss after 2 epoch is 641.4311115193988\n",
      "loss after 3 epoch is 640.9649289148044\n",
      "loss after 4 epoch is 641.1308486485443\n",
      "loss after 5 epoch is 640.7643300841419\n",
      "loss after 6 epoch is 640.8487049206835\n",
      "loss after 7 epoch is 640.5557343658287\n",
      "loss after 8 epoch is 640.5133995732969\n",
      "loss after 9 epoch is 640.4165146864816\n",
      "loss after 10 epoch is 640.5108552278233\n",
      "loss after 11 epoch is 640.2643091510052\n",
      "loss after 12 epoch is 640.1307053003084\n",
      "loss after 13 epoch is 640.228894996288\n",
      "loss after 14 epoch is 639.8572751881555\n",
      "loss after 15 epoch is 639.7491907647907\n",
      "loss after 16 epoch is 640.0449929995253\n",
      "loss after 17 epoch is 639.9056670035543\n",
      "loss after 18 epoch is 639.6249992722869\n",
      "loss after 19 epoch is 639.4413774544751\n",
      "loss after 20 epoch is 639.3121570158794\n",
      "loss after 21 epoch is 639.3480008866891\n",
      "loss after 22 epoch is 638.9809415044905\n",
      "loss after 23 epoch is 638.6854220051753\n",
      "loss after 24 epoch is 638.5616319713957\n",
      "loss after 25 epoch is 638.02780742585\n",
      "loss after 26 epoch is 637.7477194019023\n",
      "loss after 27 epoch is 637.3437917689176\n",
      "loss after 28 epoch is 636.794348941537\n",
      "loss after 29 epoch is 636.351389107965\n",
      "loss after 30 epoch is 635.16148081034\n",
      "loss after 31 epoch is 634.1549380773579\n",
      "loss after 32 epoch is 632.7564019488455\n",
      "loss after 33 epoch is 630.8502929982042\n",
      "loss after 34 epoch is 629.870913399793\n",
      "loss after 35 epoch is 623.7450546193455\n",
      "loss after 36 epoch is 618.8577704133509\n",
      "loss after 37 epoch is 648.1805612974523\n",
      "loss after 38 epoch is 645.5903379894717\n",
      "loss after 39 epoch is 643.1010519847986\n",
      "loss after 40 epoch is 640.5362113841492\n"
     ]
    }
   ],
   "source": [
    "LF=nn.CrossEntropyLoss()\n",
    "model=lstm_model(20,6,3)\n",
    "optim=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "model.to(device='cuda:0')\n",
    "LF.to(device='cuda:0')\n",
    "prev_l=0\n",
    "for epoch in range(100):\n",
    "    l=0\n",
    "    np.random.shuffle(dataset)\n",
    "    optim.zero_grad()\n",
    "    i=0\n",
    "    for each in dataset:\n",
    "        i+=1\n",
    "        pred=model(torch.from_numpy(each[0].reshape(each[0].shape[1],1,each[0].shape[0])).to(torch.double).to(device='cuda:0'))\n",
    "        pred.squeeze(dim=1)\n",
    "        label=torch.tensor([0,0,0])\n",
    "        label[each[1]]=1\n",
    "        label=label.repeat(each[0].shape[1],1)\n",
    "        label.to(pred.dtype)\n",
    "        label=label.to(device='cuda:0')\n",
    "        label=label.double()\n",
    "        pred=pred.double()\n",
    "        loss=LF(pred.squeeze(),label)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            l+=loss.item()\n",
    "        if i%10==0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "    print(\"loss after {} epoch is {}\".format(epoch,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([87, 1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (l1): LSTM(1, 6)\n",
       "  (l2): LSTM(6, 6)\n",
       "  (l3): Linear(in_features=6, out_features=3, bias=True)\n",
       "  (l4): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (l1): LSTM(1, 6)\n",
       "  (l2): LSTM(6, 6)\n",
       "  (l3): Linear(in_features=6, out_features=3, bias=True)\n",
       "  (l4): Softmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('train_clean','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()\n",
    "count=0\n",
    "for data in dataset:\n",
    "    y=model(torch.tensor(data[0].reshape(data[0].shape[0],1,1)).to(torch.double))\n",
    "    if (y.sum(dim=0)).argmax(dim=1)[0]==data[1]:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('train_clean_spec','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025, 87)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_model(nn.Module):\n",
    "    def __init__(self,input,hidden,out):\n",
    "        super(lstm_model,self).__init__()\n",
    "        self.l1=nn.LSTM(input,hidden,dtype=torch.double)\n",
    "        # self.l2=nn.LSTM(hidden,hidden,dtype=torch.double)\n",
    "        # self.l3=nn.LSTM(hidden,hidden,dtype=torch.double)\n",
    "        self.l4=nn.LSTM(hidden,hidden,dtype=torch.double)\n",
    "        self.l5=nn.Linear(hidden,out,dtype=torch.double)\n",
    "        self.l6=nn.Softmax(dim=1)\n",
    "    def forward(self,data):\n",
    "        # print(data)\n",
    "        h0,_=self.l1(data)\n",
    "        # h0,_=self.l2(h0)\n",
    "        # h0,_=self.l3(h0)\n",
    "        h0,_=self.l4(h0)\n",
    "        e=self.l5(h0[-1])\n",
    "        return self.l6(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 epoch is 64.11544234332939\n",
      "loss after 1 epoch is 64.03767734911165\n",
      "loss after 2 epoch is 64.02687677850126\n",
      "loss after 3 epoch is 63.98137041914235\n",
      "loss after 4 epoch is 63.94335064767189\n",
      "loss after 5 epoch is 63.92348765217487\n",
      "loss after 6 epoch is 63.92843915107409\n",
      "loss after 7 epoch is 63.845236507028886\n",
      "loss after 8 epoch is 63.82038182465679\n",
      "loss after 9 epoch is 63.8122842094625\n",
      "loss after 10 epoch is 63.78682670622534\n",
      "loss after 11 epoch is 63.80228025552483\n",
      "loss after 12 epoch is 63.70431878977392\n",
      "loss after 13 epoch is 63.65675141834722\n",
      "loss after 14 epoch is 63.66862787292318\n",
      "loss after 15 epoch is 63.65067431572236\n",
      "loss after 16 epoch is 63.59711145545504\n",
      "loss after 17 epoch is 63.469723696752624\n",
      "loss after 18 epoch is 63.65058706891126\n",
      "loss after 19 epoch is 63.83764865653097\n",
      "loss after 20 epoch is 63.84214306913482\n",
      "loss after 21 epoch is 63.69637330854025\n",
      "loss after 22 epoch is 63.22675172654128\n",
      "loss after 23 epoch is 63.20750879179701\n",
      "loss after 24 epoch is 63.01954969369866\n",
      "loss after 25 epoch is 63.03000819418651\n",
      "loss after 26 epoch is 62.743439182337085\n",
      "loss after 27 epoch is 62.83216691179814\n",
      "loss after 28 epoch is 62.42662759403138\n",
      "loss after 29 epoch is 63.130362479587134\n",
      "loss after 30 epoch is 63.3047311233698\n",
      "loss after 31 epoch is 63.17944994316869\n",
      "loss after 32 epoch is 62.98971974354164\n",
      "loss after 33 epoch is 62.750795554447436\n",
      "loss after 34 epoch is 62.2195442821811\n",
      "loss after 35 epoch is 61.86813265550646\n",
      "loss after 36 epoch is 61.48122893525968\n",
      "loss after 37 epoch is 61.12880546188472\n",
      "loss after 38 epoch is 60.55627404328121\n",
      "loss after 39 epoch is 60.248840192168885\n",
      "loss after 40 epoch is 59.313852982325514\n",
      "loss after 41 epoch is 58.52543587458336\n",
      "loss after 42 epoch is 57.89129037304304\n",
      "loss after 43 epoch is 56.843406603544985\n",
      "loss after 44 epoch is 55.93125700953824\n",
      "loss after 45 epoch is 55.161128684197244\n",
      "loss after 46 epoch is 61.602478541169454\n",
      "loss after 47 epoch is 66.39683484980965\n",
      "loss after 48 epoch is 65.28918763266547\n",
      "loss after 49 epoch is 61.16867258689849\n",
      "loss after 50 epoch is 59.34939015961527\n",
      "loss after 51 epoch is 58.253313305989884\n",
      "loss after 52 epoch is 64.97040307182579\n",
      "loss after 53 epoch is 64.89992551419103\n",
      "loss after 54 epoch is 64.86219099636246\n",
      "loss after 55 epoch is 64.81466059706254\n",
      "loss after 56 epoch is 64.76948452533645\n",
      "loss after 57 epoch is 64.71931686573606\n",
      "loss after 58 epoch is 64.67177599307321\n",
      "loss after 59 epoch is 64.60879201836885\n",
      "loss after 60 epoch is 64.55749863601598\n",
      "loss after 61 epoch is 64.49504014952433\n",
      "loss after 62 epoch is 64.43736435623764\n",
      "loss after 63 epoch is 64.36528292139043\n",
      "loss after 64 epoch is 64.29623389103091\n",
      "loss after 65 epoch is 64.22508924968389\n",
      "loss after 66 epoch is 64.12802258811148\n",
      "loss after 67 epoch is 63.485095309223176\n",
      "loss after 68 epoch is 63.24539968625114\n",
      "loss after 69 epoch is 64.42825490677244\n",
      "loss after 70 epoch is 63.753152317936696\n",
      "loss after 71 epoch is 62.40180244853501\n",
      "loss after 72 epoch is 64.19370343235957\n",
      "loss after 73 epoch is 64.02235680848456\n",
      "loss after 74 epoch is 63.69180171291856\n",
      "loss after 75 epoch is 62.70499765437198\n",
      "loss after 76 epoch is 64.08391979304508\n",
      "loss after 77 epoch is 64.0484234533528\n",
      "loss after 78 epoch is 61.562059001622444\n",
      "loss after 79 epoch is 59.12955172537604\n",
      "loss after 80 epoch is 61.94818304456589\n",
      "loss after 81 epoch is 64.24548709273914\n",
      "loss after 82 epoch is 64.0962501303412\n",
      "loss after 83 epoch is 63.89262289183526\n",
      "loss after 84 epoch is 63.60814110462834\n",
      "loss after 85 epoch is 62.16779275303633\n",
      "loss after 86 epoch is 64.28026412991106\n",
      "loss after 87 epoch is 64.23326004660275\n",
      "loss after 88 epoch is 64.20214922757793\n",
      "loss after 89 epoch is 64.16393827720762\n",
      "loss after 90 epoch is 64.14723190457211\n",
      "loss after 91 epoch is 64.13293404278852\n",
      "loss after 92 epoch is 64.11754671881044\n",
      "loss after 93 epoch is 64.12819187910033\n",
      "loss after 94 epoch is 64.11560518857429\n",
      "loss after 95 epoch is 64.10390490673393\n",
      "loss after 96 epoch is 64.10641081402876\n",
      "loss after 97 epoch is 64.11336620348096\n",
      "loss after 98 epoch is 64.10400646972158\n",
      "loss after 99 epoch is 64.11446934891666\n"
     ]
    }
   ],
   "source": [
    "LF=nn.CrossEntropyLoss()\n",
    "model=lstm_model(1025,10,3)\n",
    "optim=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "model.to(device='cuda:0')\n",
    "LF.to(device='cuda:0')\n",
    "fp=open('logging.txt','w')\n",
    "for epoch in range(100):\n",
    "    l=0\n",
    "    np.random.shuffle(dataset)\n",
    "    model.zero_grad()\n",
    "    i=0\n",
    "    for each in dataset:\n",
    "        i+=1\n",
    "        pred=model(torch.from_numpy(each[0].T.reshape(each[0].shape[1],1,each[0].shape[0])).to(torch.double).to(device='cuda:0'))\n",
    "        label=torch.tensor([0,0,0])\n",
    "        label[each[1]]=1\n",
    "        # label=label.repeat(each[0].shape[1],1)\n",
    "        label.to(pred.dtype)\n",
    "        label=label.to(device='cuda:0')\n",
    "        label=label.double()\n",
    "        pred=pred.double()\n",
    "        loss=0.1*LF(pred.squeeze(dim=0),label)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            l+=loss.item()\n",
    "        if i%10==0:\n",
    "            # print(model.l5.weight.grad,file=fp)\n",
    "            print(\"weights_grad_before\",model.l5.weight.grad,file=fp)\n",
    "            print(\"bias_grad_before\",model.l5.bias.grad,file=fp)\n",
    "            # print(\"weights_before\",model[0].weight)\n",
    "            optim.step()\n",
    "            # print(\"weights_after\",model.l5.weight,file=fp)\n",
    "            # print(\"weights_after\",model[0].weight)\n",
    "            model.zero_grad()\n",
    "    print(\"loss after {} epoch is {}\".format(epoch,l))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_model(\n",
       "  (l1): LSTM(1025, 10)\n",
       "  (l4): LSTM(10, 10)\n",
       "  (l5): Linear(in_features=10, out_features=3, bias=True)\n",
       "  (l6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('train_clean_spec','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()\n",
    "count=0\n",
    "ys=[]\n",
    "for data in dataset:\n",
    "    # print(data[0])\n",
    "    y=model(torch.from_numpy(each[0].T.reshape(each[0].shape[1],1,each[0].shape[0])).to(torch.double))\n",
    "    ys.append(y)\n",
    "    if y.argmax(dim=0)[0]==data[1]:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('train_clean_spec','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_model,self).__init__()\n",
    "        self.lay1=nn.Conv2d(1,6,kernel_size=3,stride=1,padding=1)\n",
    "        self.lay2=nn.ReLU()\n",
    "        self.lay2=nn.MaxPool2d(3)\n",
    "        self.lay3=nn.Conv2d(6,1,kernel_size=3,stride=1,padding=1)\n",
    "        self.lay4=nn.ReLU()\n",
    "        self.lay5=nn.MaxPool2d(3)\n",
    "        self.lay6=nn.Linear(9*113,100)\n",
    "        self.lay7=nn.Linear(100,3)\n",
    "        self.lay8=nn.Softmax(dim=1)\n",
    "    def forward(self,data):\n",
    "        x=self.lay1(data)\n",
    "        x=self.lay2(x)\n",
    "        x=self.lay3(x)\n",
    "        x=self.lay4(x)\n",
    "        x=self.lay5(x)\n",
    "        x=self.lay6(x.view(x.shape[0],-1))\n",
    "        x=self.lay7(x)\n",
    "        print(x.shape)\n",
    "        x=self.lay8(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([64, 3])\n",
      "torch.Size([8, 3])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prev_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1948\\3214916703.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mprev_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CNN_model_vowel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prev_l' is not defined"
     ]
    }
   ],
   "source": [
    "model=CNN_model()\n",
    "LF=nn.CrossEntropyLoss()\n",
    "LF.to(device='cuda:0')\n",
    "model.to(device='cuda:0')\n",
    "optim=torch.optim.SGD(model.parameters(),0.0001,momentum=0.9)\n",
    "# prev_l=np.inf\n",
    "for epoch in range(100):\n",
    "    l=0\n",
    "    for train_fea,train_lab in train_dataloader:\n",
    "        # print(train_fea.shape)\n",
    "        model.zero_grad()\n",
    "        y=model(train_fea.unsqueeze(dim=1).to(device='cuda:0'))\n",
    "        train_lab=train_lab.to(device='cuda:0')\n",
    "        loss=LF(y,train_lab)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            l+=loss.item()\n",
    "    if prev_l-l>0:\n",
    "        prev_l=l\n",
    "        torch.save(model,'CNN_model_vowel')\n",
    "    prev_l=l\n",
    "    print(\"loss after {} epoch is {}\".format(l,epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442],\n",
       "        [0.3051, 0.3507, 0.3442]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('train_clean_spect_cons','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 4.395607233047485 epoch is 0\n",
      "loss after 4.395277261734009 epoch is 1\n",
      "loss after 4.39242160320282 epoch is 2\n",
      "loss after 4.39226233959198 epoch is 3\n",
      "loss after 4.390631556510925 epoch is 4\n",
      "loss after 4.393967866897583 epoch is 5\n",
      "loss after 4.391802906990051 epoch is 6\n",
      "loss after 4.393234372138977 epoch is 7\n",
      "loss after 4.390318036079407 epoch is 8\n",
      "loss after 4.39328670501709 epoch is 9\n",
      "loss after 4.392322421073914 epoch is 10\n",
      "loss after 4.3895134925842285 epoch is 11\n",
      "loss after 4.390265345573425 epoch is 12\n",
      "loss after 4.392548084259033 epoch is 13\n",
      "loss after 4.390468955039978 epoch is 14\n",
      "loss after 4.391672134399414 epoch is 15\n",
      "loss after 4.39161229133606 epoch is 16\n",
      "loss after 4.389627933502197 epoch is 17\n",
      "loss after 4.389632344245911 epoch is 18\n",
      "loss after 4.391000390052795 epoch is 19\n",
      "loss after 4.390263319015503 epoch is 20\n",
      "loss after 4.389466881752014 epoch is 21\n",
      "loss after 4.386701345443726 epoch is 22\n",
      "loss after 4.386721849441528 epoch is 23\n",
      "loss after 4.391894221305847 epoch is 24\n",
      "loss after 4.387213230133057 epoch is 25\n",
      "loss after 4.38881254196167 epoch is 26\n",
      "loss after 4.389485716819763 epoch is 27\n",
      "loss after 4.385277032852173 epoch is 28\n",
      "loss after 4.387477874755859 epoch is 29\n",
      "loss after 4.384939908981323 epoch is 30\n",
      "loss after 4.38532280921936 epoch is 31\n",
      "loss after 4.386959075927734 epoch is 32\n",
      "loss after 4.389591097831726 epoch is 33\n",
      "loss after 4.3888840675354 epoch is 34\n",
      "loss after 4.3893723487854 epoch is 35\n",
      "loss after 4.387211799621582 epoch is 36\n",
      "loss after 4.38615608215332 epoch is 37\n",
      "loss after 4.3819814920425415 epoch is 38\n",
      "loss after 4.385497212409973 epoch is 39\n",
      "loss after 4.388393044471741 epoch is 40\n",
      "loss after 4.387651205062866 epoch is 41\n",
      "loss after 4.380957722663879 epoch is 42\n",
      "loss after 4.387267708778381 epoch is 43\n",
      "loss after 4.38654363155365 epoch is 44\n",
      "loss after 4.380867600440979 epoch is 45\n",
      "loss after 4.392321348190308 epoch is 46\n",
      "loss after 4.382646083831787 epoch is 47\n",
      "loss after 4.380138993263245 epoch is 48\n",
      "loss after 4.380275845527649 epoch is 49\n",
      "loss after 4.381085157394409 epoch is 50\n",
      "loss after 4.3841472864151 epoch is 51\n",
      "loss after 4.387051701545715 epoch is 52\n",
      "loss after 4.386715769767761 epoch is 53\n",
      "loss after 4.381052613258362 epoch is 54\n",
      "loss after 4.382257342338562 epoch is 55\n",
      "loss after 4.385213255882263 epoch is 56\n",
      "loss after 4.379793763160706 epoch is 57\n",
      "loss after 4.374448537826538 epoch is 58\n",
      "loss after 4.384718298912048 epoch is 59\n",
      "loss after 4.377798318862915 epoch is 60\n",
      "loss after 4.38173508644104 epoch is 61\n",
      "loss after 4.380993604660034 epoch is 62\n",
      "loss after 4.379560589790344 epoch is 63\n",
      "loss after 4.38858699798584 epoch is 64\n",
      "loss after 4.371443510055542 epoch is 65\n",
      "loss after 4.375986814498901 epoch is 66\n",
      "loss after 4.370419025421143 epoch is 67\n",
      "loss after 4.3790082931518555 epoch is 68\n",
      "loss after 4.362063407897949 epoch is 69\n",
      "loss after 4.3684574365615845 epoch is 70\n",
      "loss after 4.34571897983551 epoch is 71\n",
      "loss after 4.360793113708496 epoch is 72\n",
      "loss after 4.316163063049316 epoch is 73\n",
      "loss after 4.198452830314636 epoch is 74\n",
      "loss after 4.159960389137268 epoch is 75\n",
      "loss after 4.072430729866028 epoch is 76\n",
      "loss after 4.0036885142326355 epoch is 77\n",
      "loss after 3.87936794757843 epoch is 78\n",
      "loss after 3.782873034477234 epoch is 79\n",
      "loss after 3.6346254348754883 epoch is 80\n",
      "loss after 3.5538976788520813 epoch is 81\n",
      "loss after 3.4119548201560974 epoch is 82\n",
      "loss after 3.2765185236930847 epoch is 83\n",
      "loss after 3.1086058020591736 epoch is 84\n",
      "loss after 2.933825731277466 epoch is 85\n",
      "loss after 2.8184268474578857 epoch is 86\n",
      "loss after 2.7636812329292297 epoch is 87\n",
      "loss after 2.7235119342803955 epoch is 88\n",
      "loss after 2.678302824497223 epoch is 89\n",
      "loss after 2.558178186416626 epoch is 90\n",
      "loss after 2.5980018973350525 epoch is 91\n",
      "loss after 2.5533502101898193 epoch is 92\n",
      "loss after 2.5588384866714478 epoch is 93\n",
      "loss after 2.5551231503486633 epoch is 94\n",
      "loss after 2.46963369846344 epoch is 95\n",
      "loss after 2.4501891136169434 epoch is 96\n",
      "loss after 2.425387740135193 epoch is 97\n",
      "loss after 2.4811203479766846 epoch is 98\n",
      "loss after 2.427807867527008 epoch is 99\n"
     ]
    }
   ],
   "source": [
    "model=CNN_model()\n",
    "model.to(device='cuda:0')\n",
    "optim=torch.optim.SGD(model.parameters(),0.001,momentum=0.9)\n",
    "prev_l=np.inf\n",
    "for epoch in range(100):\n",
    "    l=0\n",
    "    for train_fea,train_lab in train_dataloader:\n",
    "        # print(train_fea.shape)\n",
    "        model.zero_grad()\n",
    "        y=model(train_fea.unsqueeze(dim=1).to(device='cuda:0'))\n",
    "        train_lab=train_lab.to(device='cuda:0')\n",
    "        loss=LF(y,train_lab)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            l+=loss.item()\n",
    "    if prev_l-l>0:\n",
    "        prev_l=l\n",
    "        torch.save(model,'CNN_model_consonant')\n",
    "    print(\"loss after {} epoch is {}\".format(l,epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('D:\\codes\\M.Tech_proj\\CNN_model_consonant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (lay1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lay2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lay3): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lay4): ReLU()\n",
       "  (lay5): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (lay6): Linear(in_features=1017, out_features=100, bias=True)\n",
       "  (lay7): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (lay8): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('D:/codes/M.Tech_proj/test_clean_spect_cons','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys=[]\n",
    "count=0\n",
    "for fea,label in train_dataloader:\n",
    "    # print(data[0])\n",
    "    y=model(fea.unsqueeze(dim=1))\n",
    "    for i,each in enumerate(y.argmax(dim=1)):\n",
    "        if each==label[i]:\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "np.random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(dataset[i][0])\n",
    "print(dataset[i][1])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[i][0])\n",
    "print(dataset[i][1])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.imshow(dataset[i][0])\n",
    "print(dataset[i][1])\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_model,self).__init__()\n",
    "        self.lay1=nn.Conv2d(1,6,kernel_size=3,stride=1,padding=1)\n",
    "        self.lay2=nn.ReLU()\n",
    "        self.lay3=nn.Conv2d(6,1,kernel_size=3,stride=1,padding=1)\n",
    "        self.lay3=nn.Conv2d(1,6,kernel_size=3,stride=1,padding=1)\n",
    "        self.lay3=nn.Conv2d(6,1,kernel_size=3,stride=1,padding=1)\n",
    "        self.lay4=nn.ReLU()\n",
    "        self.lay6=nn.Linear(20*87,100)\n",
    "        self.lay7=nn.Linear(100,3)\n",
    "        self.lay8=nn.Softmax(dim=1)\n",
    "    def forward(self,data):\n",
    "        x=self.lay1(data)\n",
    "        x=self.lay2(x)\n",
    "        x=self.lay3(x)\n",
    "        x=self.lay4(x)\n",
    "        # print(\"after flatten\",x.shape)\n",
    "        x=self.lay6(x.view(x.shape[0],-1))\n",
    "        x=self.lay7(x)\n",
    "        x=self.lay8(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "fp=open('D:/codes/M.Tech_proj/train_clean_mfcc_cons','rb')\n",
    "dataset=pickle.load(fp)\n",
    "X=[0]*len(dataset)\n",
    "x_train,x_test,y_train,y_test=train_test_split(dataset,X,test_size=0.2,train_size=0.8,random_state=4)\n",
    "fp.close()\n",
    "train_dataloader = DataLoader(x_train, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(x_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 epcoh train 5.512387752532959 and valid 2.121208667755127\n",
      "after 1 epcoh train 5.384271144866943 and valid 2.069042921066284\n",
      "after 2 epcoh train 4.939950942993164 and valid 1.64150071144104\n",
      "after 3 epcoh train 4.569672107696533 and valid 1.6800031661987305\n",
      "after 4 epcoh train 4.265960693359375 and valid 1.9446868896484375\n",
      "after 5 epcoh train 4.033776760101318 and valid 1.715843677520752\n",
      "after 6 epcoh train 4.041426658630371 and valid 1.5786895751953125\n",
      "after 7 epcoh train 3.775714159011841 and valid 1.4754350185394287\n",
      "after 8 epcoh train 3.8787922859191895 and valid 1.270172357559204\n",
      "after 9 epcoh train 3.7437686920166016 and valid 1.4959595203399658\n",
      "after 10 epcoh train 3.63527250289917 and valid 1.3939297199249268\n",
      "after 11 epcoh train 3.732287883758545 and valid 1.2527060508728027\n",
      "after 12 epcoh train 3.560194969177246 and valid 1.490911841392517\n",
      "after 13 epcoh train 3.4498817920684814 and valid 1.1881206035614014\n",
      "after 14 epcoh train 3.5618643760681152 and valid 1.4922447204589844\n",
      "after 15 epcoh train 4.714560508728027 and valid 1.3638967275619507\n",
      "after 16 epcoh train 3.842003107070923 and valid 1.4424062967300415\n",
      "after 17 epcoh train 4.487864017486572 and valid 1.3919599056243896\n",
      "after 18 epcoh train 4.754676342010498 and valid 1.362337589263916\n",
      "after 19 epcoh train 3.716146230697632 and valid 1.4909859895706177\n",
      "after 20 epcoh train 3.642660140991211 and valid 1.462593913078308\n",
      "after 21 epcoh train 3.511204242706299 and valid 1.2979252338409424\n",
      "after 22 epcoh train 3.702359199523926 and valid 1.22114896774292\n",
      "after 23 epcoh train 3.3607661724090576 and valid 1.176999568939209\n",
      "after 24 epcoh train 3.3588809967041016 and valid 1.210042953491211\n",
      "after 25 epcoh train 3.525486946105957 and valid 1.212571620941162\n",
      "after 26 epcoh train 3.222451686859131 and valid 1.144850492477417\n",
      "after 27 epcoh train 3.368075370788574 and valid 1.1560101509094238\n",
      "after 28 epcoh train 3.4487757682800293 and valid 1.14308500289917\n",
      "after 29 epcoh train 3.323989152908325 and valid 1.176871418952942\n",
      "after 30 epcoh train 3.2502505779266357 and valid 1.1648101806640625\n",
      "after 31 epcoh train 3.3098301887512207 and valid 1.1421570777893066\n",
      "after 32 epcoh train 3.1938650608062744 and valid 1.1469389200210571\n",
      "after 33 epcoh train 3.4228153228759766 and valid 1.159643530845642\n",
      "after 34 epcoh train 3.2998085021972656 and valid 1.1451280117034912\n",
      "after 35 epcoh train 3.3952465057373047 and valid 1.4428012371063232\n",
      "after 36 epcoh train 3.344104290008545 and valid 1.1569204330444336\n",
      "after 37 epcoh train 3.1741280555725098 and valid 1.1460829973220825\n",
      "after 38 epcoh train 3.250739812850952 and valid 1.3769972324371338\n",
      "after 39 epcoh train 3.180230140686035 and valid 1.1402587890625\n",
      "after 40 epcoh train 3.1775479316711426 and valid 1.1383004188537598\n",
      "after 41 epcoh train 3.2435152530670166 and valid 1.1429862976074219\n",
      "after 42 epcoh train 3.162818193435669 and valid 1.137277364730835\n",
      "after 43 epcoh train 3.0985336303710938 and valid 1.1368811130523682\n",
      "after 44 epcoh train 3.1276133060455322 and valid 1.1344698667526245\n",
      "after 45 epcoh train 3.0940022468566895 and valid 1.1366567611694336\n",
      "after 46 epcoh train 3.2266933917999268 and valid 1.5902431011199951\n",
      "after 47 epcoh train 3.2069036960601807 and valid 1.1438443660736084\n",
      "after 48 epcoh train 3.4554708003997803 and valid 1.3936361074447632\n",
      "after 49 epcoh train 3.33463454246521 and valid 1.1303476095199585\n",
      "after 50 epcoh train 3.2224936485290527 and valid 1.1676843166351318\n",
      "after 51 epcoh train 3.1204681396484375 and valid 1.3732950687408447\n",
      "after 52 epcoh train 3.102574110031128 and valid 1.1372319459915161\n",
      "after 53 epcoh train 3.0974953174591064 and valid 1.1207046508789062\n",
      "after 54 epcoh train 3.187472343444824 and valid 1.1744091510772705\n",
      "after 55 epcoh train 3.292414903640747 and valid 1.3556175231933594\n",
      "after 56 epcoh train 3.1345174312591553 and valid 1.127347469329834\n",
      "after 57 epcoh train 3.0947797298431396 and valid 1.2808666229248047\n",
      "after 58 epcoh train 3.6609854698181152 and valid 1.3050594329833984\n",
      "after 59 epcoh train 4.2102508544921875 and valid 1.217584490776062\n",
      "after 60 epcoh train 3.496624708175659 and valid 1.165543794631958\n",
      "after 61 epcoh train 3.513197422027588 and valid 1.2466074228286743\n",
      "after 62 epcoh train 3.391425609588623 and valid 1.1492822170257568\n",
      "after 63 epcoh train 3.299781322479248 and valid 1.1795635223388672\n",
      "after 64 epcoh train 3.1309382915496826 and valid 1.14823579788208\n",
      "after 65 epcoh train 3.144198417663574 and valid 1.158775806427002\n",
      "after 66 epcoh train 3.135958671569824 and valid 1.1335718631744385\n",
      "after 67 epcoh train 3.122424602508545 and valid 1.174861192703247\n",
      "after 68 epcoh train 3.3044559955596924 and valid 1.1962802410125732\n",
      "after 69 epcoh train 3.062333106994629 and valid 1.1323988437652588\n",
      "after 70 epcoh train 3.2364232540130615 and valid 1.135361909866333\n",
      "after 71 epcoh train 3.1968162059783936 and valid 1.232006311416626\n",
      "after 72 epcoh train 3.0738062858581543 and valid 1.2542057037353516\n",
      "after 73 epcoh train 3.6743764877319336 and valid 1.1748783588409424\n",
      "after 74 epcoh train 4.018307209014893 and valid 1.1542632579803467\n",
      "after 75 epcoh train 3.358530044555664 and valid 1.1483049392700195\n",
      "after 76 epcoh train 3.3003575801849365 and valid 1.2009696960449219\n",
      "after 77 epcoh train 3.208719491958618 and valid 1.1528736352920532\n",
      "after 78 epcoh train 3.1763811111450195 and valid 1.119415521621704\n",
      "after 79 epcoh train 3.2213339805603027 and valid 1.355093240737915\n",
      "after 80 epcoh train 3.6682140827178955 and valid 1.1876885890960693\n",
      "after 81 epcoh train 3.3001549243927 and valid 1.3064429759979248\n",
      "after 82 epcoh train 3.3446455001831055 and valid 1.17026948928833\n",
      "after 83 epcoh train 3.2575559616088867 and valid 1.1190359592437744\n",
      "after 84 epcoh train 3.135739326477051 and valid 1.1205649375915527\n",
      "after 85 epcoh train 3.1414949893951416 and valid 1.140437364578247\n",
      "after 86 epcoh train 3.1040074825286865 and valid 1.139434576034546\n",
      "after 87 epcoh train 3.210022211074829 and valid 1.1198831796646118\n",
      "after 88 epcoh train 3.308670997619629 and valid 1.1237437725067139\n",
      "after 89 epcoh train 3.0780980587005615 and valid 1.1227974891662598\n",
      "after 90 epcoh train 3.138024091720581 and valid 1.1197173595428467\n",
      "after 91 epcoh train 3.138859748840332 and valid 1.1189196109771729\n",
      "after 92 epcoh train 3.1348109245300293 and valid 1.1186647415161133\n",
      "after 93 epcoh train 3.1278138160705566 and valid 1.118668794631958\n",
      "after 94 epcoh train 3.065062999725342 and valid 1.11885666847229\n",
      "after 95 epcoh train 3.125138998031616 and valid 1.1186888217926025\n",
      "after 96 epcoh train 3.063444137573242 and valid 1.1190004348754883\n",
      "after 97 epcoh train 3.0456643104553223 and valid 1.123946189880371\n",
      "after 98 epcoh train 3.104085683822632 and valid 1.1188828945159912\n",
      "after 99 epcoh train 3.098684310913086 and valid 1.120126724243164\n",
      "after 100 epcoh train 3.1383721828460693 and valid 1.1289374828338623\n",
      "after 101 epcoh train 3.017733573913574 and valid 1.1253986358642578\n",
      "after 102 epcoh train 2.9992876052856445 and valid 1.1231224536895752\n",
      "after 103 epcoh train 3.062239646911621 and valid 1.12088942527771\n",
      "after 104 epcoh train 2.9947807788848877 and valid 1.1192512512207031\n",
      "after 105 epcoh train 3.111849308013916 and valid 1.2997453212738037\n",
      "after 106 epcoh train 3.536856174468994 and valid 1.1885302066802979\n",
      "after 107 epcoh train 3.2971701622009277 and valid 1.1207094192504883\n",
      "after 108 epcoh train 3.089026927947998 and valid 1.118538737297058\n",
      "after 109 epcoh train 3.0279581546783447 and valid 1.3864142894744873\n",
      "after 110 epcoh train 3.008803606033325 and valid 1.1521769762039185\n",
      "after 111 epcoh train 3.056562662124634 and valid 1.1244609355926514\n",
      "after 112 epcoh train 2.9788818359375 and valid 1.3559231758117676\n",
      "after 113 epcoh train 3.0560014247894287 and valid 1.1326451301574707\n",
      "after 114 epcoh train 2.9516661167144775 and valid 1.1251246929168701\n",
      "after 115 epcoh train 2.9386777877807617 and valid 1.1196763515472412\n",
      "after 116 epcoh train 2.9045250415802 and valid 1.1192822456359863\n",
      "after 117 epcoh train 2.869216203689575 and valid 1.1195650100708008\n",
      "after 118 epcoh train 2.9506194591522217 and valid 1.1227216720581055\n",
      "after 119 epcoh train 2.976484775543213 and valid 1.1270993947982788\n",
      "after 120 epcoh train 2.9994029998779297 and valid 1.118850588798523\n",
      "after 121 epcoh train 2.9932403564453125 and valid 1.1186432838439941\n",
      "after 122 epcoh train 2.890401601791382 and valid 1.1194119453430176\n",
      "after 123 epcoh train 3.0233945846557617 and valid 1.1225833892822266\n",
      "after 124 epcoh train 3.1025257110595703 and valid 1.1279234886169434\n",
      "after 125 epcoh train 2.9059243202209473 and valid 1.157099723815918\n",
      "after 126 epcoh train 2.921504259109497 and valid 1.1380903720855713\n",
      "after 127 epcoh train 2.8611602783203125 and valid 1.133150577545166\n",
      "after 128 epcoh train 2.8403782844543457 and valid 1.1217737197875977\n",
      "after 129 epcoh train 2.8138973712921143 and valid 1.123690128326416\n",
      "after 130 epcoh train 2.814244270324707 and valid 1.3542969226837158\n",
      "after 131 epcoh train 2.8122379779815674 and valid 1.1192827224731445\n",
      "after 132 epcoh train 2.8082447052001953 and valid 1.11873459815979\n",
      "after 133 epcoh train 2.807447671890259 and valid 1.118668556213379\n",
      "after 134 epcoh train 2.874671697616577 and valid 1.1186444759368896\n",
      "after 135 epcoh train 2.8060426712036133 and valid 1.118660569190979\n",
      "after 136 epcoh train 2.8062055110931396 and valid 1.1186797618865967\n",
      "after 137 epcoh train 2.8733596801757812 and valid 1.1187150478363037\n",
      "after 138 epcoh train 2.805600166320801 and valid 1.1186819076538086\n",
      "after 139 epcoh train 2.805570125579834 and valid 1.1186609268188477\n",
      "after 140 epcoh train 2.8056085109710693 and valid 1.1186431646347046\n",
      "after 141 epcoh train 2.8059029579162598 and valid 1.1190767288208008\n",
      "after 142 epcoh train 2.805432081222534 and valid 1.1189285516738892\n",
      "after 143 epcoh train 2.8054237365722656 and valid 1.1186652183532715\n",
      "after 144 epcoh train 2.8053536415100098 and valid 1.118628978729248\n",
      "after 145 epcoh train 2.805316925048828 and valid 1.1186219453811646\n",
      "after 146 epcoh train 2.805375814437866 and valid 1.118625283241272\n",
      "after 147 epcoh train 2.8053715229034424 and valid 1.1186213493347168\n",
      "after 148 epcoh train 2.805344343185425 and valid 1.1186208724975586\n",
      "after 149 epcoh train 2.8053550720214844 and valid 1.1186283826828003\n",
      "after 150 epcoh train 2.8058855533599854 and valid 1.1186282634735107\n",
      "after 151 epcoh train 2.8058159351348877 and valid 1.118647575378418\n",
      "after 152 epcoh train 2.8054072856903076 and valid 1.1186225414276123\n",
      "after 153 epcoh train 2.8051340579986572 and valid 1.1186256408691406\n",
      "after 154 epcoh train 2.805665969848633 and valid 1.1186178922653198\n",
      "after 155 epcoh train 2.872737169265747 and valid 1.1186296939849854\n",
      "after 156 epcoh train 2.8051140308380127 and valid 1.118636131286621\n",
      "after 157 epcoh train 2.804990768432617 and valid 1.1186583042144775\n",
      "after 158 epcoh train 2.8049798011779785 and valid 1.1186308860778809\n",
      "after 159 epcoh train 2.805612802505493 and valid 1.118621826171875\n",
      "after 160 epcoh train 2.8725273609161377 and valid 1.1186168193817139\n",
      "after 161 epcoh train 2.872523784637451 and valid 1.1187188625335693\n",
      "after 162 epcoh train 2.872537612915039 and valid 1.1186033487319946\n",
      "after 163 epcoh train 2.804948568344116 and valid 1.1185979843139648\n",
      "after 164 epcoh train 2.804931163787842 and valid 1.118598461151123\n",
      "after 165 epcoh train 2.8726017475128174 and valid 1.1185957193374634\n",
      "after 166 epcoh train 2.8048739433288574 and valid 1.1185970306396484\n",
      "after 167 epcoh train 2.8049185276031494 and valid 1.1185975074768066\n",
      "after 168 epcoh train 2.873016834259033 and valid 1.1186003684997559\n",
      "after 169 epcoh train 2.804964542388916 and valid 1.352980136871338\n",
      "after 170 epcoh train 2.804961681365967 and valid 1.1186137199401855\n",
      "after 171 epcoh train 2.8730874061584473 and valid 1.1186163425445557\n",
      "after 172 epcoh train 2.8048107624053955 and valid 1.1186212301254272\n",
      "after 173 epcoh train 2.8727707862854004 and valid 1.1186716556549072\n",
      "after 174 epcoh train 2.8047778606414795 and valid 1.118607521057129\n",
      "after 175 epcoh train 2.8048720359802246 and valid 1.1186003684997559\n",
      "after 176 epcoh train 2.8047521114349365 and valid 1.11863112449646\n",
      "after 177 epcoh train 2.804746389389038 and valid 1.1186047792434692\n",
      "after 178 epcoh train 2.8047409057617188 and valid 1.1185939311981201\n",
      "after 179 epcoh train 2.804816961288452 and valid 1.1186137199401855\n",
      "after 180 epcoh train 2.8724708557128906 and valid 1.1188571453094482\n",
      "after 181 epcoh train 2.804720163345337 and valid 1.118768572807312\n",
      "after 182 epcoh train 2.8047244548797607 and valid 1.118626356124878\n",
      "after 183 epcoh train 2.872401714324951 and valid 1.1186299324035645\n",
      "after 184 epcoh train 2.8724167346954346 and valid 1.1186714172363281\n",
      "after 185 epcoh train 2.872462272644043 and valid 1.1186014413833618\n",
      "after 186 epcoh train 2.805109977722168 and valid 1.1186236143112183\n",
      "after 187 epcoh train 2.8046817779541016 and valid 1.1185872554779053\n",
      "after 188 epcoh train 2.8046860694885254 and valid 1.1185846328735352\n",
      "after 189 epcoh train 2.804672956466675 and valid 1.1185834407806396\n",
      "after 190 epcoh train 2.8723459243774414 and valid 1.1189236640930176\n",
      "after 191 epcoh train 2.8723607063293457 and valid 1.1192708015441895\n",
      "after 192 epcoh train 2.804670572280884 and valid 1.1185715198516846\n",
      "after 193 epcoh train 2.8047962188720703 and valid 1.1185694932937622\n",
      "after 194 epcoh train 2.8048219680786133 and valid 1.1185686588287354\n",
      "after 195 epcoh train 2.8047101497650146 and valid 1.1185649633407593\n",
      "after 196 epcoh train 2.8048245906829834 and valid 1.1185616254806519\n",
      "after 197 epcoh train 2.8048255443573 and valid 1.118562936782837\n",
      "after 198 epcoh train 2.8049445152282715 and valid 1.1185717582702637\n",
      "after 199 epcoh train 2.8046340942382812 and valid 1.1188037395477295\n",
      "after 200 epcoh train 2.804633617401123 and valid 1.118564248085022\n",
      "after 201 epcoh train 2.8048899173736572 and valid 1.118854284286499\n",
      "after 202 epcoh train 2.804828643798828 and valid 1.1185636520385742\n",
      "after 203 epcoh train 2.8049280643463135 and valid 1.1185598373413086\n",
      "after 204 epcoh train 2.8047008514404297 and valid 1.1185601949691772\n",
      "after 205 epcoh train 2.804863929748535 and valid 1.1187669038772583\n",
      "after 206 epcoh train 2.804799795150757 and valid 1.1185650825500488\n",
      "after 207 epcoh train 2.8048391342163086 and valid 1.1189677715301514\n",
      "after 208 epcoh train 2.8045668601989746 and valid 1.1185612678527832\n",
      "after 209 epcoh train 2.804624080657959 and valid 1.1185615062713623\n",
      "after 210 epcoh train 2.872467041015625 and valid 1.11856210231781\n",
      "after 211 epcoh train 2.804600477218628 and valid 1.1185765266418457\n",
      "after 212 epcoh train 2.8048720359802246 and valid 1.1185617446899414\n",
      "after 213 epcoh train 2.804619789123535 and valid 1.118716835975647\n",
      "after 214 epcoh train 2.8045527935028076 and valid 1.1186654567718506\n",
      "after 215 epcoh train 2.8722424507141113 and valid 1.1185657978057861\n",
      "after 216 epcoh train 2.8047008514404297 and valid 1.1185697317123413\n",
      "after 217 epcoh train 2.804532051086426 and valid 1.3529471158981323\n",
      "after 218 epcoh train 2.804532051086426 and valid 1.3529458045959473\n",
      "after 219 epcoh train 2.804624319076538 and valid 1.1185691356658936\n",
      "after 220 epcoh train 2.8046445846557617 and valid 1.1185674667358398\n",
      "after 221 epcoh train 2.804610252380371 and valid 1.1185904741287231\n",
      "after 222 epcoh train 2.8045451641082764 and valid 1.118835210800171\n",
      "after 223 epcoh train 2.804515838623047 and valid 1.1185598373413086\n",
      "after 224 epcoh train 2.8048219680786133 and valid 1.118566632270813\n",
      "after 225 epcoh train 2.804680824279785 and valid 1.1185534000396729\n",
      "after 226 epcoh train 2.804539442062378 and valid 1.1185554265975952\n",
      "after 227 epcoh train 2.8045482635498047 and valid 1.1185557842254639\n",
      "after 228 epcoh train 2.8722646236419678 and valid 1.1185559034347534\n",
      "after 229 epcoh train 2.804637908935547 and valid 1.1185557842254639\n",
      "after 230 epcoh train 2.804508924484253 and valid 1.352929711341858\n",
      "after 231 epcoh train 2.8045005798339844 and valid 1.118553638458252\n",
      "after 232 epcoh train 2.8721911907196045 and valid 1.1185531616210938\n",
      "after 233 epcoh train 2.872195243835449 and valid 1.1185532808303833\n",
      "after 234 epcoh train 2.8045458793640137 and valid 1.1187138557434082\n",
      "after 235 epcoh train 2.8046271800994873 and valid 1.1186506748199463\n",
      "after 236 epcoh train 2.8721988201141357 and valid 1.1185622215270996\n",
      "after 237 epcoh train 2.8045899868011475 and valid 1.1186981201171875\n",
      "after 238 epcoh train 2.804638385772705 and valid 1.1185476779937744\n",
      "after 239 epcoh train 2.804682731628418 and valid 1.1185486316680908\n",
      "after 240 epcoh train 2.804469108581543 and valid 1.1185498237609863\n",
      "after 241 epcoh train 2.80446720123291 and valid 1.1185667514801025\n",
      "after 242 epcoh train 2.872255325317383 and valid 1.1185671091079712\n",
      "after 243 epcoh train 2.872166395187378 and valid 1.1185507774353027\n",
      "after 244 epcoh train 2.8721461296081543 and valid 1.3529305458068848\n",
      "after 245 epcoh train 2.804471969604492 and valid 1.1185503005981445\n",
      "after 246 epcoh train 2.8046655654907227 and valid 1.1185503005981445\n",
      "after 247 epcoh train 2.804457664489746 and valid 1.1185503005981445\n",
      "after 248 epcoh train 2.8045382499694824 and valid 1.1185503005981445\n",
      "after 249 epcoh train 2.804448366165161 and valid 1.1185510158538818\n",
      "after 250 epcoh train 2.8044488430023193 and valid 1.1185508966445923\n",
      "after 251 epcoh train 2.8044917583465576 and valid 1.1187901496887207\n",
      "after 252 epcoh train 2.804457664489746 and valid 1.1185640096664429\n",
      "after 253 epcoh train 2.8722803592681885 and valid 1.1185472011566162\n",
      "after 254 epcoh train 2.8044846057891846 and valid 1.1185463666915894\n",
      "after 255 epcoh train 2.8044445514678955 and valid 1.118546485900879\n",
      "after 256 epcoh train 2.804478883743286 and valid 1.118546485900879\n",
      "after 257 epcoh train 2.8044395446777344 and valid 1.1185486316680908\n",
      "after 258 epcoh train 2.8044445514678955 and valid 1.1185479164123535\n",
      "after 259 epcoh train 2.8721444606781006 and valid 1.1185482740402222\n",
      "after 260 epcoh train 2.804492473602295 and valid 1.1185526847839355\n",
      "after 261 epcoh train 2.8044273853302 and valid 1.1185486316680908\n",
      "after 262 epcoh train 2.872281074523926 and valid 1.1185722351074219\n",
      "after 263 epcoh train 2.8045036792755127 and valid 1.11854887008667\n",
      "after 264 epcoh train 2.8044211864471436 and valid 1.1185494661331177\n",
      "after 265 epcoh train 2.80444073677063 and valid 1.1185688972473145\n",
      "after 266 epcoh train 2.8044378757476807 and valid 1.1185503005981445\n",
      "after 267 epcoh train 2.8044400215148926 and valid 1.118717074394226\n",
      "after 268 epcoh train 2.804553508758545 and valid 1.353182315826416\n",
      "after 269 epcoh train 2.8044638633728027 and valid 1.1185438632965088\n",
      "after 270 epcoh train 2.804544687271118 and valid 1.1185420751571655\n",
      "after 271 epcoh train 2.8044726848602295 and valid 1.1185414791107178\n",
      "after 272 epcoh train 2.8044326305389404 and valid 1.1185415983200073\n",
      "after 273 epcoh train 2.8044180870056152 and valid 1.1185457706451416\n",
      "after 274 epcoh train 2.804412603378296 and valid 1.1185423135757446\n",
      "after 275 epcoh train 2.8044164180755615 and valid 1.1185534000396729\n",
      "after 276 epcoh train 2.8044228553771973 and valid 1.1185526847839355\n",
      "after 277 epcoh train 2.8044118881225586 and valid 1.1185429096221924\n",
      "after 278 epcoh train 2.8044044971466064 and valid 1.118543267250061\n",
      "after 279 epcoh train 2.8044421672821045 and valid 1.1185433864593506\n",
      "after 280 epcoh train 2.8045217990875244 and valid 1.1185438632965088\n",
      "after 281 epcoh train 2.804440498352051 and valid 1.1186120510101318\n",
      "after 282 epcoh train 2.804473638534546 and valid 1.1185449361801147\n",
      "after 283 epcoh train 2.8045055866241455 and valid 1.1185672283172607\n",
      "after 284 epcoh train 2.8043956756591797 and valid 1.1186935901641846\n",
      "after 285 epcoh train 2.804394483566284 and valid 1.1185466051101685\n",
      "after 286 epcoh train 2.804398775100708 and valid 1.1185415983200073\n",
      "after 287 epcoh train 2.804399013519287 and valid 1.1185543537139893\n",
      "after 288 epcoh train 2.872121572494507 and valid 1.118542194366455\n",
      "after 289 epcoh train 2.80446457862854 and valid 1.1185470819473267\n",
      "after 290 epcoh train 2.804391860961914 and valid 1.1185433864593506\n",
      "after 291 epcoh train 2.8043971061706543 and valid 1.1185483932495117\n",
      "after 292 epcoh train 2.8044769763946533 and valid 1.1185418367385864\n",
      "after 293 epcoh train 2.8721065521240234 and valid 1.1185581684112549\n",
      "after 294 epcoh train 2.804384469985962 and valid 1.118542194366455\n",
      "after 295 epcoh train 2.8043808937072754 and valid 1.1185424327850342\n",
      "after 296 epcoh train 2.804394006729126 and valid 1.1187424659729004\n",
      "after 297 epcoh train 2.804435968399048 and valid 1.118542194366455\n",
      "after 298 epcoh train 2.8043861389160156 and valid 1.1185404062271118\n",
      "after 299 epcoh train 2.804379940032959 and valid 1.1185400485992432\n",
      "after 300 epcoh train 2.8044447898864746 and valid 1.1185998916625977\n",
      "after 301 epcoh train 2.8043792247772217 and valid 1.1185412406921387\n",
      "after 302 epcoh train 2.872093677520752 and valid 1.1185417175292969\n",
      "after 303 epcoh train 2.804414749145508 and valid 1.35296630859375\n",
      "after 304 epcoh train 2.8044686317443848 and valid 1.11857008934021\n",
      "after 305 epcoh train 2.804368734359741 and valid 1.1185433864593506\n",
      "after 306 epcoh train 2.804415464401245 and valid 1.1185543537139893\n",
      "after 307 epcoh train 2.804372549057007 and valid 1.1185476779937744\n",
      "after 308 epcoh train 2.804368734359741 and valid 1.1185426712036133\n",
      "after 309 epcoh train 2.8044774532318115 and valid 1.1185425519943237\n",
      "after 310 epcoh train 2.804474115371704 and valid 1.118543028831482\n",
      "after 311 epcoh train 2.8720645904541016 and valid 1.1185625791549683\n",
      "after 312 epcoh train 2.872110605239868 and valid 1.1186697483062744\n",
      "after 313 epcoh train 2.8043606281280518 and valid 1.1186165809631348\n",
      "after 314 epcoh train 2.804457187652588 and valid 1.1185402870178223\n",
      "after 315 epcoh train 2.8043665885925293 and valid 1.1185531616210938\n",
      "after 316 epcoh train 2.8043696880340576 and valid 1.1185405254364014\n",
      "after 317 epcoh train 2.8043668270111084 and valid 1.1185405254364014\n",
      "after 318 epcoh train 2.804391622543335 and valid 1.1185410022735596\n",
      "after 319 epcoh train 2.8044750690460205 and valid 1.118539571762085\n",
      "after 320 epcoh train 2.8044469356536865 and valid 1.1185407638549805\n",
      "after 321 epcoh train 2.804377317428589 and valid 1.1185524463653564\n",
      "after 322 epcoh train 2.804450035095215 and valid 1.1185405254364014\n",
      "after 323 epcoh train 2.804396152496338 and valid 1.1185405254364014\n",
      "after 324 epcoh train 2.8043580055236816 and valid 1.118544578552246\n",
      "after 325 epcoh train 2.804349660873413 and valid 1.118544340133667\n",
      "after 326 epcoh train 2.8044655323028564 and valid 1.1185460090637207\n",
      "after 327 epcoh train 2.8043923377990723 and valid 1.1185414791107178\n",
      "after 328 epcoh train 2.8044686317443848 and valid 1.1185418367385864\n",
      "after 329 epcoh train 2.8043692111968994 and valid 1.118553638458252\n",
      "after 330 epcoh train 2.804351806640625 and valid 1.118542194366455\n",
      "after 331 epcoh train 2.8043415546417236 and valid 1.1185415983200073\n",
      "after 332 epcoh train 2.8043460845947266 and valid 1.1187446117401123\n",
      "after 333 epcoh train 2.80439829826355 and valid 1.1185407638549805\n",
      "after 334 epcoh train 2.8043460845947266 and valid 1.1185386180877686\n",
      "after 335 epcoh train 2.8044583797454834 and valid 1.1185383796691895\n",
      "after 336 epcoh train 2.8043923377990723 and valid 1.3529131412506104\n",
      "after 337 epcoh train 2.8720481395721436 and valid 1.1185383796691895\n",
      "after 338 epcoh train 2.8043813705444336 and valid 1.1185569763183594\n",
      "after 339 epcoh train 2.804335355758667 and valid 1.1186976432800293\n",
      "after 340 epcoh train 2.804347038269043 and valid 1.1185405254364014\n",
      "after 341 epcoh train 2.8043899536132812 and valid 1.1185375452041626\n",
      "after 342 epcoh train 2.804347276687622 and valid 1.118544578552246\n",
      "after 343 epcoh train 2.8043463230133057 and valid 1.118537187576294\n",
      "after 344 epcoh train 2.804366111755371 and valid 1.118537425994873\n",
      "after 345 epcoh train 2.8043432235717773 and valid 1.1185376644134521\n",
      "after 346 epcoh train 2.8043835163116455 and valid 1.1186449527740479\n",
      "after 347 epcoh train 2.8043742179870605 and valid 1.1185414791107178\n",
      "after 348 epcoh train 2.804335355758667 and valid 1.1186859607696533\n",
      "after 349 epcoh train 2.872129201889038 and valid 1.1185479164123535\n",
      "after 350 epcoh train 2.8043465614318848 and valid 1.118535041809082\n",
      "after 351 epcoh train 2.804344892501831 and valid 1.118535041809082\n",
      "after 352 epcoh train 2.87203049659729 and valid 1.118664026260376\n",
      "after 353 epcoh train 2.804368734359741 and valid 1.1185345649719238\n",
      "after 354 epcoh train 2.8044002056121826 and valid 1.118546724319458\n",
      "after 355 epcoh train 2.8043529987335205 and valid 1.1185344457626343\n",
      "after 356 epcoh train 2.804328203201294 and valid 1.118534803390503\n",
      "after 357 epcoh train 2.8044087886810303 and valid 1.1185352802276611\n",
      "after 358 epcoh train 2.8044042587280273 and valid 1.1185352802276611\n",
      "after 359 epcoh train 2.804394245147705 and valid 1.1185355186462402\n",
      "after 360 epcoh train 2.8043768405914307 and valid 1.1185364723205566\n",
      "after 361 epcoh train 2.8043205738067627 and valid 1.118536353111267\n",
      "after 362 epcoh train 2.8720662593841553 and valid 1.1185367107391357\n",
      "after 363 epcoh train 2.80434513092041 and valid 1.1185386180877686\n",
      "after 364 epcoh train 2.804316282272339 and valid 1.1186426877975464\n",
      "after 365 epcoh train 2.8043439388275146 and valid 1.1185452938079834\n",
      "after 366 epcoh train 2.804372787475586 and valid 1.1185493469238281\n",
      "after 367 epcoh train 2.8720130920410156 and valid 1.352912187576294\n",
      "after 368 epcoh train 2.8043923377990723 and valid 1.1185357570648193\n",
      "after 369 epcoh train 2.8043463230133057 and valid 1.1186003684997559\n",
      "after 370 epcoh train 2.804316997528076 and valid 1.1185402870178223\n",
      "after 371 epcoh train 2.8043160438537598 and valid 1.1185848712921143\n",
      "after 372 epcoh train 2.804387331008911 and valid 1.1185375452041626\n",
      "after 373 epcoh train 2.8044373989105225 and valid 1.3529129028320312\n",
      "after 374 epcoh train 2.804389238357544 and valid 1.1185429096221924\n",
      "after 375 epcoh train 2.8043134212493896 and valid 1.3529142141342163\n",
      "after 376 epcoh train 2.8043313026428223 and valid 1.1185399293899536\n",
      "after 377 epcoh train 2.804309844970703 and valid 1.1185394525527954\n",
      "after 378 epcoh train 2.80438494682312 and valid 1.1185393333435059\n",
      "after 379 epcoh train 2.804335117340088 and valid 1.1185508966445923\n",
      "after 380 epcoh train 2.804318428039551 and valid 1.1185429096221924\n",
      "after 381 epcoh train 2.8044207096099854 and valid 1.1186493635177612\n",
      "after 382 epcoh train 2.8044276237487793 and valid 1.118541955947876\n",
      "after 383 epcoh train 2.804370164871216 and valid 1.1185377836227417\n",
      "after 384 epcoh train 2.804304361343384 and valid 1.1185362339019775\n",
      "after 385 epcoh train 2.804330587387085 and valid 1.1185365915298462\n",
      "after 386 epcoh train 2.804302215576172 and valid 1.1185462474822998\n",
      "after 387 epcoh train 2.804346799850464 and valid 1.1186940670013428\n",
      "after 388 epcoh train 2.8043019771575928 and valid 1.1185355186462402\n",
      "after 389 epcoh train 2.8043081760406494 and valid 1.1185343265533447\n",
      "after 390 epcoh train 2.8043205738067627 and valid 1.118534803390503\n",
      "after 391 epcoh train 2.804349184036255 and valid 1.118534803390503\n",
      "after 392 epcoh train 2.804356098175049 and valid 1.1185404062271118\n",
      "after 393 epcoh train 2.8043768405914307 and valid 1.3529119491577148\n",
      "after 394 epcoh train 2.8043155670166016 and valid 1.1186718940734863\n",
      "after 395 epcoh train 2.8043272495269775 and valid 1.1185352802276611\n",
      "after 396 epcoh train 2.8043694496154785 and valid 1.1185345649719238\n",
      "after 397 epcoh train 2.8042967319488525 and valid 1.1185346841812134\n",
      "after 398 epcoh train 2.8043782711029053 and valid 1.1185346841812134\n",
      "after 399 epcoh train 2.8043580055236816 and valid 1.1185352802276611\n",
      "after 400 epcoh train 2.8720786571502686 and valid 1.1185424327850342\n",
      "after 401 epcoh train 2.804321765899658 and valid 1.1185537576675415\n",
      "after 402 epcoh train 2.8043158054351807 and valid 1.1185755729675293\n",
      "after 403 epcoh train 2.804305076599121 and valid 1.1185376644134521\n",
      "after 404 epcoh train 2.804290294647217 and valid 1.3529108762741089\n",
      "after 405 epcoh train 2.8043127059936523 and valid 1.118540644645691\n",
      "after 406 epcoh train 2.804293632507324 and valid 1.118537425994873\n",
      "after 407 epcoh train 2.804323434829712 and valid 1.1185373067855835\n",
      "after 408 epcoh train 2.8044381141662598 and valid 1.1185462474822998\n",
      "after 409 epcoh train 2.8043010234832764 and valid 1.1185357570648193\n",
      "after 410 epcoh train 2.8719944953918457 and valid 1.118649959564209\n",
      "after 411 epcoh train 2.8042850494384766 and valid 1.1185357570648193\n",
      "after 412 epcoh train 2.804365634918213 and valid 1.1185345649719238\n",
      "after 413 epcoh train 2.8043177127838135 and valid 1.118533730506897\n",
      "after 414 epcoh train 2.8043296337127686 and valid 1.1185362339019775\n",
      "after 415 epcoh train 2.804286241531372 and valid 1.1185336112976074\n",
      "after 416 epcoh train 2.8043062686920166 and valid 1.1185364723205566\n",
      "after 417 epcoh train 2.8043124675750732 and valid 1.1185338497161865\n",
      "after 418 epcoh train 2.804286479949951 and valid 1.1186071634292603\n",
      "after 419 epcoh train 2.8043696880340576 and valid 1.1185358762741089\n",
      "after 420 epcoh train 2.8043923377990723 and valid 1.1185359954833984\n",
      "after 421 epcoh train 2.8042843341827393 and valid 1.1185338497161865\n",
      "after 422 epcoh train 2.8043439388275146 and valid 1.118546485900879\n",
      "after 423 epcoh train 2.8043060302734375 and valid 1.1185345649719238\n",
      "after 424 epcoh train 2.804281711578369 and valid 1.1185345649719238\n",
      "after 425 epcoh train 2.8721649646759033 and valid 1.1185377836227417\n",
      "after 426 epcoh train 2.804356813430786 and valid 1.3529198169708252\n",
      "after 427 epcoh train 2.8042783737182617 and valid 1.1186747550964355\n",
      "after 428 epcoh train 2.8719828128814697 and valid 1.1185340881347656\n",
      "after 429 epcoh train 2.8043198585510254 and valid 1.3529243469238281\n",
      "after 430 epcoh train 2.8042900562286377 and valid 1.118550419807434\n",
      "after 431 epcoh train 2.804325819015503 and valid 1.1185331344604492\n",
      "after 432 epcoh train 2.8043088912963867 and valid 1.1185333728790283\n",
      "after 433 epcoh train 2.8043696880340576 and valid 1.1185336112976074\n",
      "after 434 epcoh train 2.8042795658111572 and valid 1.1185352802276611\n",
      "after 435 epcoh train 2.8042781352996826 and valid 1.1185362339019775\n",
      "after 436 epcoh train 2.8042736053466797 and valid 1.1185338497161865\n",
      "after 437 epcoh train 2.804283618927002 and valid 1.118549108505249\n",
      "after 438 epcoh train 2.8042900562286377 and valid 1.118535041809082\n",
      "after 439 epcoh train 2.804324150085449 and valid 1.1185338497161865\n",
      "after 440 epcoh train 2.8042783737182617 and valid 1.1185340881347656\n",
      "after 441 epcoh train 2.8720107078552246 and valid 1.1185342073440552\n",
      "after 442 epcoh train 2.8719911575317383 and valid 1.1185375452041626\n",
      "after 443 epcoh train 2.8043224811553955 and valid 1.1186678409576416\n",
      "after 444 epcoh train 2.8042871952056885 and valid 1.1185351610183716\n",
      "after 445 epcoh train 2.8042855262756348 and valid 1.118534803390503\n",
      "after 446 epcoh train 2.804314136505127 and valid 1.1185357570648193\n",
      "after 447 epcoh train 2.8042876720428467 and valid 1.1185333728790283\n",
      "after 448 epcoh train 2.804269313812256 and valid 1.1185352802276611\n",
      "after 449 epcoh train 2.8043060302734375 and valid 1.118572473526001\n",
      "after 450 epcoh train 2.80429744720459 and valid 1.1185364723205566\n",
      "after 451 epcoh train 2.8042829036712646 and valid 1.1185342073440552\n",
      "after 452 epcoh train 2.804337501525879 and valid 1.1186370849609375\n",
      "after 453 epcoh train 2.804286003112793 and valid 1.1185572147369385\n",
      "after 454 epcoh train 2.804325819015503 and valid 1.118532419204712\n",
      "after 455 epcoh train 2.8043060302734375 and valid 1.1185336112976074\n",
      "after 456 epcoh train 2.804290771484375 and valid 1.118532419204712\n",
      "after 457 epcoh train 2.8042922019958496 and valid 1.1185352802276611\n",
      "after 458 epcoh train 2.872034788131714 and valid 1.1185327768325806\n",
      "after 459 epcoh train 2.8042852878570557 and valid 1.1185948848724365\n",
      "after 460 epcoh train 2.8042681217193604 and valid 1.1185327768325806\n",
      "after 461 epcoh train 2.8720033168792725 and valid 1.118532657623291\n",
      "after 462 epcoh train 2.8042707443237305 and valid 1.1185331344604492\n",
      "after 463 epcoh train 2.804292678833008 and valid 1.118532657623291\n",
      "after 464 epcoh train 2.8043129444122314 and valid 1.118535041809082\n",
      "after 465 epcoh train 2.80426287651062 and valid 1.1185328960418701\n",
      "after 466 epcoh train 2.8042757511138916 and valid 1.1185336112976074\n",
      "after 467 epcoh train 2.8042798042297363 and valid 1.1186001300811768\n",
      "after 468 epcoh train 2.804263114929199 and valid 1.118532657623291\n",
      "after 469 epcoh train 2.804415464401245 and valid 1.118671178817749\n",
      "after 470 epcoh train 2.804262161254883 and valid 1.118531584739685\n",
      "after 471 epcoh train 2.804264783859253 and valid 1.3529062271118164\n",
      "after 472 epcoh train 2.8042919635772705 and valid 1.118531346321106\n",
      "after 473 epcoh train 2.804264783859253 and valid 1.1185314655303955\n",
      "after 474 epcoh train 2.871971368789673 and valid 1.1185364723205566\n",
      "after 475 epcoh train 2.804323196411133 and valid 1.1185331344604492\n",
      "after 476 epcoh train 2.804262638092041 and valid 1.1185319423675537\n",
      "after 477 epcoh train 2.8042614459991455 and valid 1.1185321807861328\n",
      "after 478 epcoh train 2.8720502853393555 and valid 1.1185340881347656\n",
      "after 479 epcoh train 2.804281711578369 and valid 1.1185336112976074\n",
      "after 480 epcoh train 2.8042654991149902 and valid 1.1185340881347656\n",
      "after 481 epcoh train 2.8042593002319336 and valid 1.1185731887817383\n",
      "after 482 epcoh train 2.804297685623169 and valid 1.1185336112976074\n",
      "after 483 epcoh train 2.8042521476745605 and valid 1.1185681819915771\n",
      "after 484 epcoh train 2.8719592094421387 and valid 1.118534803390503\n",
      "after 485 epcoh train 2.804251194000244 and valid 1.118645191192627\n",
      "after 486 epcoh train 2.8042664527893066 and valid 1.1185336112976074\n",
      "after 487 epcoh train 2.80425763130188 and valid 1.118532657623291\n",
      "after 488 epcoh train 2.80431866645813 and valid 1.118572473526001\n",
      "after 489 epcoh train 2.8042984008789062 and valid 1.1185328960418701\n",
      "after 490 epcoh train 2.8719639778137207 and valid 1.1185333728790283\n",
      "after 491 epcoh train 2.8043627738952637 and valid 1.1185334920883179\n",
      "after 492 epcoh train 2.804305076599121 and valid 1.1185369491577148\n",
      "after 493 epcoh train 2.8043112754821777 and valid 1.1185427904129028\n",
      "after 494 epcoh train 2.8043580055236816 and valid 1.1185345649719238\n",
      "after 495 epcoh train 2.8042640686035156 and valid 1.118553638458252\n",
      "after 496 epcoh train 2.8042516708374023 and valid 1.1185349225997925\n",
      "after 497 epcoh train 2.804265260696411 and valid 1.1185451745986938\n",
      "after 498 epcoh train 2.8720779418945312 and valid 1.1185340881347656\n",
      "after 499 epcoh train 2.8043816089630127 and valid 1.1185340881347656\n",
      "after 500 epcoh train 2.8042562007904053 and valid 1.1185368299484253\n",
      "after 501 epcoh train 2.8042688369750977 and valid 1.1185344457626343\n",
      "after 502 epcoh train 2.8042471408843994 and valid 1.1185353994369507\n",
      "after 503 epcoh train 2.8719544410705566 and valid 1.1185388565063477\n",
      "after 504 epcoh train 2.804248809814453 and valid 1.1186672449111938\n",
      "after 505 epcoh train 2.8042454719543457 and valid 1.1185393333435059\n",
      "after 506 epcoh train 2.8042452335357666 and valid 1.1185327768325806\n",
      "after 507 epcoh train 2.8043112754821777 and valid 1.1186211109161377\n",
      "after 508 epcoh train 2.8042478561401367 and valid 1.11854088306427\n",
      "after 509 epcoh train 2.804270029067993 and valid 1.118531584739685\n",
      "after 510 epcoh train 2.8043410778045654 and valid 1.11857271194458\n",
      "after 511 epcoh train 2.8042819499969482 and valid 1.1185302734375\n",
      "after 512 epcoh train 2.804250955581665 and valid 1.1185338497161865\n",
      "after 513 epcoh train 2.8720381259918213 and valid 1.1185457706451416\n",
      "after 514 epcoh train 2.8042755126953125 and valid 1.1185327768325806\n",
      "after 515 epcoh train 2.8042609691619873 and valid 1.1185312271118164\n",
      "after 516 epcoh train 2.8042426109313965 and valid 1.118531346321106\n",
      "after 517 epcoh train 2.8042993545532227 and valid 1.118531346321106\n",
      "after 518 epcoh train 2.8042619228363037 and valid 1.1185483932495117\n",
      "after 519 epcoh train 2.804241180419922 and valid 1.1185321807861328\n",
      "after 520 epcoh train 2.804246664047241 and valid 1.1185317039489746\n",
      "after 521 epcoh train 2.8719475269317627 and valid 1.1185331344604492\n",
      "after 522 epcoh train 2.8043296337127686 and valid 1.1185345649719238\n",
      "after 523 epcoh train 2.8042750358581543 and valid 1.1185320615768433\n",
      "after 524 epcoh train 2.804239273071289 and valid 1.1185336112976074\n",
      "after 525 epcoh train 2.804335117340088 and valid 1.1185344457626343\n",
      "after 526 epcoh train 2.871945858001709 and valid 1.1185345649719238\n",
      "after 527 epcoh train 2.8042755126953125 and valid 1.1185325384140015\n",
      "after 528 epcoh train 2.8042855262756348 and valid 1.118532657623291\n",
      "after 529 epcoh train 2.8042898178100586 and valid 1.118553638458252\n",
      "after 530 epcoh train 2.871962547302246 and valid 1.1185500621795654\n",
      "after 531 epcoh train 2.8042566776275635 and valid 1.118647575378418\n",
      "after 532 epcoh train 2.8042373657226562 and valid 1.1185312271118164\n",
      "after 533 epcoh train 2.8042426109313965 and valid 1.1185308694839478\n",
      "after 534 epcoh train 2.8042545318603516 and valid 1.1186283826828003\n",
      "after 535 epcoh train 2.8042490482330322 and valid 1.3529052734375\n",
      "after 536 epcoh train 2.8042917251586914 and valid 1.1185312271118164\n",
      "after 537 epcoh train 2.8042385578155518 and valid 1.1185314655303955\n",
      "after 538 epcoh train 2.80423903465271 and valid 1.118530511856079\n",
      "after 539 epcoh train 2.804238796234131 and valid 1.1185307502746582\n",
      "after 540 epcoh train 2.804237127304077 and valid 1.1185307502746582\n",
      "after 541 epcoh train 2.8042356967926025 and valid 1.1185309886932373\n",
      "after 542 epcoh train 2.804272174835205 and valid 1.1185317039489746\n",
      "after 543 epcoh train 2.8042352199554443 and valid 1.1185318231582642\n",
      "after 544 epcoh train 2.8042335510253906 and valid 1.118532657623291\n",
      "after 545 epcoh train 2.871929883956909 and valid 1.1185623407363892\n",
      "after 546 epcoh train 2.8042328357696533 and valid 1.1185317039489746\n",
      "after 547 epcoh train 2.8042471408843994 and valid 1.3529119491577148\n",
      "after 548 epcoh train 2.804231643676758 and valid 1.1185320615768433\n",
      "after 549 epcoh train 2.804283857345581 and valid 1.1185600757598877\n",
      "after 550 epcoh train 2.8042540550231934 and valid 1.1185598373413086\n",
      "after 551 epcoh train 2.804269313812256 and valid 1.1186325550079346\n",
      "after 552 epcoh train 2.8042445182800293 and valid 1.1185314655303955\n",
      "after 553 epcoh train 2.804234266281128 and valid 1.1185457706451416\n",
      "after 554 epcoh train 2.8719284534454346 and valid 1.118530511856079\n",
      "after 555 epcoh train 2.804232120513916 and valid 1.1185303926467896\n",
      "after 556 epcoh train 2.804309606552124 and valid 1.1185412406921387\n",
      "after 557 epcoh train 2.8042349815368652 and valid 1.1185317039489746\n",
      "after 558 epcoh train 2.80428409576416 and valid 1.118636965751648\n",
      "after 559 epcoh train 2.804260730743408 and valid 1.118544578552246\n",
      "after 560 epcoh train 2.8719401359558105 and valid 1.1185927391052246\n",
      "after 561 epcoh train 2.8042514324188232 and valid 1.1185286045074463\n",
      "after 562 epcoh train 2.939636707305908 and valid 1.1185286045074463\n",
      "after 563 epcoh train 2.804236650466919 and valid 1.118537187576294\n",
      "after 564 epcoh train 2.804239273071289 and valid 1.1185284852981567\n",
      "after 565 epcoh train 2.804234027862549 and valid 1.1185657978057861\n",
      "after 566 epcoh train 2.8042492866516113 and valid 1.1186245679855347\n",
      "after 567 epcoh train 2.804234027862549 and valid 1.1185288429260254\n",
      "after 568 epcoh train 2.8042469024658203 and valid 1.1185293197631836\n",
      "after 569 epcoh train 2.8043031692504883 and valid 1.118592619895935\n",
      "after 570 epcoh train 2.8042314052581787 and valid 1.1185657978057861\n",
      "after 571 epcoh train 2.804287910461426 and valid 1.1185295581817627\n",
      "after 572 epcoh train 2.8042352199554443 and valid 1.1185288429260254\n",
      "after 573 epcoh train 2.939634084701538 and valid 1.1185294389724731\n",
      "after 574 epcoh train 2.8042492866516113 and valid 1.118530035018921\n",
      "after 575 epcoh train 2.8042550086975098 and valid 1.1185373067855835\n",
      "after 576 epcoh train 2.804324150085449 and valid 1.1185288429260254\n",
      "after 577 epcoh train 2.8042383193969727 and valid 1.118529200553894\n",
      "after 578 epcoh train 2.804255723953247 and valid 1.1185297966003418\n",
      "after 579 epcoh train 2.8042263984680176 and valid 1.1185295581817627\n",
      "after 580 epcoh train 2.8043251037597656 and valid 1.1185941696166992\n",
      "after 581 epcoh train 2.8042726516723633 and valid 1.1185290813446045\n",
      "after 582 epcoh train 2.8719446659088135 and valid 1.1185288429260254\n",
      "after 583 epcoh train 2.804241180419922 and valid 1.1185303926467896\n",
      "after 584 epcoh train 2.804232597351074 and valid 1.1185288429260254\n",
      "after 585 epcoh train 2.8042335510253906 and valid 1.1185302734375\n",
      "after 586 epcoh train 2.8042304515838623 and valid 1.1185290813446045\n",
      "after 587 epcoh train 2.8042402267456055 and valid 1.1185290813446045\n",
      "after 588 epcoh train 2.8042855262756348 and valid 1.1185930967330933\n",
      "after 589 epcoh train 2.804248571395874 and valid 1.1185314655303955\n",
      "after 590 epcoh train 2.804230213165283 and valid 1.1185290813446045\n",
      "after 591 epcoh train 2.8042526245117188 and valid 1.1185729503631592\n",
      "after 592 epcoh train 2.804225444793701 and valid 1.1185288429260254\n",
      "after 593 epcoh train 2.871947765350342 and valid 1.1185288429260254\n",
      "after 594 epcoh train 2.8043017387390137 and valid 1.1185288429260254\n",
      "after 595 epcoh train 2.804309368133545 and valid 1.1185288429260254\n",
      "after 596 epcoh train 2.871932029724121 and valid 1.1186699867248535\n",
      "after 597 epcoh train 2.804238796234131 and valid 1.1185322999954224\n",
      "after 598 epcoh train 2.8042755126953125 and valid 1.1185293197631836\n",
      "after 599 epcoh train 2.80428147315979 and valid 1.3529045581817627\n",
      "after 600 epcoh train 2.8042397499084473 and valid 1.1185297966003418\n",
      "after 601 epcoh train 2.8042190074920654 and valid 1.118530035018921\n",
      "after 602 epcoh train 2.871915817260742 and valid 1.1185317039489746\n",
      "after 603 epcoh train 2.804288387298584 and valid 1.1185302734375\n",
      "after 604 epcoh train 2.804234027862549 and valid 1.118532657623291\n",
      "after 605 epcoh train 2.8042213916778564 and valid 1.1185302734375\n",
      "after 606 epcoh train 2.804241180419922 and valid 1.1185302734375\n",
      "after 607 epcoh train 2.8042454719543457 and valid 1.118553638458252\n",
      "after 608 epcoh train 2.8042376041412354 and valid 1.1186232566833496\n",
      "after 609 epcoh train 2.804217576980591 and valid 1.1185302734375\n",
      "after 610 epcoh train 2.8042266368865967 and valid 1.1186178922653198\n",
      "after 611 epcoh train 2.804232358932495 and valid 1.1185557842254639\n",
      "after 612 epcoh train 2.804222583770752 and valid 1.3529032468795776\n",
      "after 613 epcoh train 2.87192964553833 and valid 1.1185295581817627\n",
      "after 614 epcoh train 2.8042712211608887 and valid 1.1185290813446045\n",
      "after 615 epcoh train 2.804244041442871 and valid 1.118544101715088\n",
      "after 616 epcoh train 2.80422306060791 and valid 1.1185306310653687\n",
      "after 617 epcoh train 2.80424427986145 and valid 1.118528127670288\n",
      "after 618 epcoh train 2.80421781539917 and valid 1.1185839176177979\n",
      "after 619 epcoh train 2.8042476177215576 and valid 1.1186134815216064\n",
      "after 620 epcoh train 2.8042356967926025 and valid 1.1185272932052612\n",
      "after 621 epcoh train 2.8043105602264404 and valid 1.1185271739959717\n",
      "after 622 epcoh train 2.8042516708374023 and valid 1.1185274124145508\n",
      "after 623 epcoh train 2.8719303607940674 and valid 1.1185274124145508\n",
      "after 624 epcoh train 2.8042445182800293 and valid 1.3529107570648193\n",
      "after 625 epcoh train 2.804267406463623 and valid 1.118528962135315\n",
      "after 626 epcoh train 2.804232597351074 and valid 1.1185276508331299\n",
      "after 627 epcoh train 2.80423903465271 and valid 1.118527889251709\n",
      "after 628 epcoh train 2.8042163848876953 and valid 1.118527889251709\n",
      "after 629 epcoh train 2.8719611167907715 and valid 1.1185288429260254\n",
      "after 630 epcoh train 2.804264783859253 and valid 1.1185306310653687\n",
      "after 631 epcoh train 2.804232120513916 and valid 1.1185461282730103\n",
      "after 632 epcoh train 2.8719213008880615 and valid 1.1185288429260254\n",
      "after 633 epcoh train 2.804267644882202 and valid 1.118537425994873\n",
      "after 634 epcoh train 2.8042805194854736 and valid 1.352966070175171\n",
      "after 635 epcoh train 2.8042776584625244 and valid 1.1185283660888672\n",
      "after 636 epcoh train 2.804229497909546 and valid 1.118532419204712\n",
      "after 637 epcoh train 2.804213762283325 and valid 1.118625521659851\n",
      "after 638 epcoh train 2.8042399883270264 and valid 1.1185415983200073\n",
      "after 639 epcoh train 2.871922254562378 and valid 1.1185274124145508\n",
      "after 640 epcoh train 2.804252862930298 and valid 1.118528127670288\n",
      "after 641 epcoh train 2.8042681217193604 and valid 1.1185276508331299\n",
      "after 642 epcoh train 2.804274082183838 and valid 1.1185288429260254\n",
      "after 643 epcoh train 2.804215908050537 and valid 1.118530511856079\n",
      "after 644 epcoh train 2.8042683601379395 and valid 1.1185293197631836\n",
      "after 645 epcoh train 2.804226875305176 and valid 1.118528962135315\n",
      "after 646 epcoh train 2.8042142391204834 and valid 1.1185293197631836\n",
      "after 647 epcoh train 2.8042283058166504 and valid 1.1185297966003418\n",
      "after 648 epcoh train 2.804219961166382 and valid 1.1185321807861328\n",
      "after 649 epcoh train 2.8720104694366455 and valid 1.118530511856079\n",
      "after 650 epcoh train 2.804220199584961 and valid 1.1185287237167358\n",
      "after 651 epcoh train 2.8042654991149902 and valid 1.1185311079025269\n",
      "after 652 epcoh train 2.804215908050537 and valid 1.118682861328125\n",
      "after 653 epcoh train 2.8042445182800293 and valid 1.1185309886932373\n",
      "after 654 epcoh train 2.8042452335357666 and valid 1.1185277700424194\n",
      "after 655 epcoh train 2.8042118549346924 and valid 1.1185276508331299\n",
      "after 656 epcoh train 2.8719282150268555 and valid 1.1185286045074463\n",
      "after 657 epcoh train 2.8042099475860596 and valid 1.1186081171035767\n",
      "after 658 epcoh train 2.8042285442352295 and valid 1.1185274124145508\n",
      "after 659 epcoh train 2.8042094707489014 and valid 1.1185283660888672\n",
      "after 660 epcoh train 2.8042092323303223 and valid 1.1185280084609985\n",
      "after 661 epcoh train 2.804220199584961 and valid 1.118598222732544\n",
      "after 662 epcoh train 2.8042702674865723 and valid 1.1185932159423828\n",
      "after 663 epcoh train 2.8042097091674805 and valid 1.1185269355773926\n",
      "after 664 epcoh train 2.804274082183838 and valid 1.118544340133667\n",
      "after 665 epcoh train 2.8042125701904297 and valid 1.118585228919983\n",
      "after 666 epcoh train 2.8042430877685547 and valid 1.1185266971588135\n",
      "after 667 epcoh train 2.804220199584961 and valid 1.1185276508331299\n",
      "after 668 epcoh train 2.8042283058166504 and valid 1.1185269355773926\n",
      "after 669 epcoh train 2.8042197227478027 and valid 1.1185269355773926\n",
      "after 670 epcoh train 2.804266929626465 and valid 1.1185917854309082\n",
      "after 671 epcoh train 2.8042140007019043 and valid 1.1185264587402344\n",
      "after 672 epcoh train 2.804227828979492 and valid 1.1185262203216553\n",
      "after 673 epcoh train 2.80424427986145 and valid 1.1185263395309448\n",
      "after 674 epcoh train 2.8042452335357666 and valid 1.1185263395309448\n",
      "after 675 epcoh train 2.8042075634002686 and valid 1.118527889251709\n",
      "after 676 epcoh train 2.871938943862915 and valid 1.1185266971588135\n",
      "after 677 epcoh train 2.804267406463623 and valid 1.1185282468795776\n",
      "after 678 epcoh train 2.871936559677124 and valid 1.3529020547866821\n",
      "after 679 epcoh train 2.8042173385620117 and valid 1.1185293197631836\n",
      "after 680 epcoh train 2.804206132888794 and valid 1.1185271739959717\n",
      "after 681 epcoh train 2.8042185306549072 and valid 1.1185271739959717\n",
      "after 682 epcoh train 2.8042051792144775 and valid 1.352902889251709\n",
      "after 683 epcoh train 2.8042051792144775 and valid 1.3529561758041382\n",
      "after 684 epcoh train 2.8042237758636475 and valid 1.3529019355773926\n",
      "after 685 epcoh train 2.804230213165283 and valid 1.1185269355773926\n",
      "after 686 epcoh train 2.8042173385620117 and valid 1.118528127670288\n",
      "after 687 epcoh train 2.804267406463623 and valid 1.1185270547866821\n",
      "after 688 epcoh train 2.8042445182800293 and valid 1.1185269355773926\n",
      "after 689 epcoh train 2.8042125701904297 and valid 1.3529019355773926\n",
      "after 690 epcoh train 2.8042047023773193 and valid 1.1185270547866821\n",
      "after 691 epcoh train 2.8042173385620117 and valid 1.1185271739959717\n",
      "after 692 epcoh train 2.804206371307373 and valid 1.118528127670288\n",
      "after 693 epcoh train 2.871896505355835 and valid 1.3529291152954102\n",
      "after 694 epcoh train 2.804241895675659 and valid 1.1185288429260254\n",
      "after 695 epcoh train 2.804231882095337 and valid 1.1186068058013916\n",
      "after 696 epcoh train 2.8042118549346924 and valid 1.1185276508331299\n",
      "after 697 epcoh train 2.8042399883270264 and valid 1.1185314655303955\n",
      "after 698 epcoh train 2.8042149543762207 and valid 1.1185309886932373\n",
      "after 699 epcoh train 2.804279327392578 and valid 1.1185299158096313\n",
      "after 700 epcoh train 2.8042044639587402 and valid 1.1185276508331299\n",
      "after 701 epcoh train 2.804257869720459 and valid 1.1185276508331299\n",
      "after 702 epcoh train 2.804237127304077 and valid 1.1185357570648193\n",
      "after 703 epcoh train 2.8042221069335938 and valid 1.3529043197631836\n",
      "after 704 epcoh train 2.8042588233947754 and valid 1.1185290813446045\n",
      "after 705 epcoh train 2.8042256832122803 and valid 1.1185293197631836\n",
      "after 706 epcoh train 2.8042216300964355 and valid 1.1185287237167358\n",
      "after 707 epcoh train 2.804231643676758 and valid 1.118602991104126\n",
      "after 708 epcoh train 2.804279088973999 and valid 1.1185284852981567\n",
      "after 709 epcoh train 2.8042197227478027 and valid 1.118577241897583\n",
      "after 710 epcoh train 2.8042116165161133 and valid 1.3529016971588135\n",
      "after 711 epcoh train 2.80423641204834 and valid 1.1185264587402344\n",
      "after 712 epcoh train 2.804201602935791 and valid 1.1185264587402344\n",
      "after 713 epcoh train 2.8042140007019043 and valid 1.1185266971588135\n",
      "after 714 epcoh train 2.8042380809783936 and valid 1.1185649633407593\n",
      "after 715 epcoh train 2.8042337894439697 and valid 1.3529016971588135\n",
      "after 716 epcoh train 2.804260730743408 and valid 1.118526816368103\n",
      "after 717 epcoh train 2.8042383193969727 and valid 1.118526816368103\n",
      "after 718 epcoh train 2.804199695587158 and valid 1.1186091899871826\n",
      "after 719 epcoh train 2.804227113723755 and valid 1.1185274124145508\n",
      "after 720 epcoh train 2.8719069957733154 and valid 1.1185553073883057\n",
      "after 721 epcoh train 2.8042445182800293 and valid 1.1185269355773926\n",
      "after 722 epcoh train 2.8042373657226562 and valid 1.1185270547866821\n",
      "after 723 epcoh train 2.804198741912842 and valid 1.1185272932052612\n",
      "after 724 epcoh train 2.804230213165283 and valid 1.1185274124145508\n",
      "after 725 epcoh train 2.8042304515838623 and valid 1.1185276508331299\n",
      "after 726 epcoh train 2.804199695587158 and valid 1.1185286045074463\n",
      "after 727 epcoh train 2.804255723953247 and valid 1.1185276508331299\n",
      "after 728 epcoh train 2.8042469024658203 and valid 1.1186081171035767\n",
      "after 729 epcoh train 2.8041999340057373 and valid 1.1185274124145508\n",
      "after 730 epcoh train 2.8041958808898926 and valid 1.1185736656188965\n",
      "after 731 epcoh train 2.8042075634002686 and valid 1.118526816368103\n",
      "after 732 epcoh train 2.804197072982788 and valid 1.118561863899231\n",
      "after 733 epcoh train 2.871940851211548 and valid 1.1185264587402344\n",
      "after 734 epcoh train 2.804208755493164 and valid 1.1185269355773926\n",
      "after 735 epcoh train 2.804229736328125 and valid 1.1186103820800781\n",
      "after 736 epcoh train 2.8042402267456055 and valid 1.1185262203216553\n",
      "after 737 epcoh train 2.87188982963562 and valid 1.1185981035232544\n",
      "after 738 epcoh train 2.871898889541626 and valid 1.118525743484497\n",
      "after 739 epcoh train 2.8042190074920654 and valid 1.118525743484497\n",
      "after 740 epcoh train 2.8041980266571045 and valid 1.352900743484497\n",
      "after 741 epcoh train 2.8042211532592773 and valid 1.1185296773910522\n",
      "after 742 epcoh train 2.8719236850738525 and valid 1.1185293197631836\n",
      "after 743 epcoh train 2.8041951656341553 and valid 1.1185263395309448\n",
      "after 744 epcoh train 2.804226875305176 and valid 1.1185264587402344\n",
      "after 745 epcoh train 2.804208517074585 and valid 1.1185343265533447\n",
      "after 746 epcoh train 2.8041958808898926 and valid 1.1185277700424194\n",
      "after 747 epcoh train 2.804231882095337 and valid 1.11855149269104\n",
      "after 748 epcoh train 2.8042283058166504 and valid 1.1185269355773926\n",
      "after 749 epcoh train 2.8041927814483643 and valid 1.1185877323150635\n",
      "after 750 epcoh train 2.8042569160461426 and valid 1.1185457706451416\n",
      "after 751 epcoh train 2.804201602935791 and valid 1.1185274124145508\n",
      "after 752 epcoh train 2.8041958808898926 and valid 1.1185277700424194\n",
      "after 753 epcoh train 2.8042070865631104 and valid 1.1185276508331299\n",
      "after 754 epcoh train 2.804194450378418 and valid 1.1185296773910522\n",
      "after 755 epcoh train 2.804192543029785 and valid 1.3529093265533447\n",
      "after 756 epcoh train 2.9396212100982666 and valid 1.1185266971588135\n",
      "after 757 epcoh train 2.804196357727051 and valid 1.1185269355773926\n",
      "after 758 epcoh train 2.8042564392089844 and valid 1.1185269355773926\n",
      "after 759 epcoh train 2.804199457168579 and valid 1.1185271739959717\n",
      "after 760 epcoh train 2.804215431213379 and valid 1.1185274124145508\n",
      "after 761 epcoh train 2.8041939735412598 and valid 1.1185274124145508\n",
      "after 762 epcoh train 2.8042073249816895 and valid 1.1185286045074463\n",
      "after 763 epcoh train 2.804201126098633 and valid 1.1185272932052612\n",
      "after 764 epcoh train 2.871891975402832 and valid 1.1185283660888672\n",
      "after 765 epcoh train 2.8042004108428955 and valid 1.1185276508331299\n",
      "after 766 epcoh train 2.804243326187134 and valid 1.1185283660888672\n",
      "after 767 epcoh train 2.8719546794891357 and valid 1.1185277700424194\n",
      "after 768 epcoh train 2.804208993911743 and valid 1.1185493469238281\n",
      "after 769 epcoh train 2.804199457168579 and valid 1.1186058521270752\n",
      "after 770 epcoh train 2.804203748703003 and valid 1.1185282468795776\n",
      "after 771 epcoh train 2.804237127304077 and valid 1.118527889251709\n",
      "after 772 epcoh train 2.871920347213745 and valid 1.1185270547866821\n",
      "after 773 epcoh train 2.8718974590301514 and valid 1.1185276508331299\n",
      "after 774 epcoh train 2.804204225540161 and valid 1.1185269355773926\n",
      "after 775 epcoh train 2.8042094707489014 and valid 1.1185476779937744\n",
      "after 776 epcoh train 2.8042750358581543 and valid 1.118546962738037\n",
      "after 777 epcoh train 2.871884822845459 and valid 1.1185276508331299\n",
      "after 778 epcoh train 2.804191827774048 and valid 1.118544101715088\n",
      "after 779 epcoh train 2.8042337894439697 and valid 1.1185269355773926\n",
      "after 780 epcoh train 2.804246425628662 and valid 1.1185274124145508\n",
      "after 781 epcoh train 2.8041884899139404 and valid 1.1185271739959717\n",
      "after 782 epcoh train 2.8042166233062744 and valid 1.1185271739959717\n",
      "after 783 epcoh train 2.8042399883270264 and valid 1.1185271739959717\n",
      "after 784 epcoh train 2.8041903972625732 and valid 1.1185264587402344\n",
      "after 785 epcoh train 2.804187774658203 and valid 1.1185264587402344\n",
      "after 786 epcoh train 2.804203987121582 and valid 1.1185274124145508\n",
      "after 787 epcoh train 2.804192543029785 and valid 1.1185274124145508\n",
      "after 788 epcoh train 2.8041887283325195 and valid 1.1185457706451416\n",
      "after 789 epcoh train 2.804187774658203 and valid 1.1185264587402344\n",
      "after 790 epcoh train 2.80423641204834 and valid 1.1185333728790283\n",
      "after 791 epcoh train 2.8041865825653076 and valid 1.1185266971588135\n",
      "after 792 epcoh train 2.8719325065612793 and valid 1.1185466051101685\n",
      "after 793 epcoh train 2.8041939735412598 and valid 1.118526816368103\n",
      "after 794 epcoh train 2.8042008876800537 and valid 1.118530035018921\n",
      "after 795 epcoh train 2.8042306900024414 and valid 1.1185280084609985\n",
      "after 796 epcoh train 2.8041863441467285 and valid 1.1185275316238403\n",
      "after 797 epcoh train 2.8041889667510986 and valid 1.1185269355773926\n",
      "after 798 epcoh train 2.804220676422119 and valid 1.1186027526855469\n",
      "after 799 epcoh train 2.804222583770752 and valid 1.1185266971588135\n",
      "after 800 epcoh train 2.8042080402374268 and valid 1.1185340881347656\n",
      "after 801 epcoh train 2.804185152053833 and valid 1.1185269355773926\n",
      "after 802 epcoh train 2.8042185306549072 and valid 1.1185269355773926\n",
      "after 803 epcoh train 2.8042118549346924 and valid 1.1185269355773926\n",
      "after 804 epcoh train 2.8042123317718506 and valid 1.1185271739959717\n",
      "after 805 epcoh train 2.804185152053833 and valid 1.1185282468795776\n",
      "after 806 epcoh train 2.8042287826538086 and valid 1.1185276508331299\n",
      "after 807 epcoh train 2.8041911125183105 and valid 1.1185283660888672\n",
      "after 808 epcoh train 2.8042216300964355 and valid 1.1185286045074463\n",
      "after 809 epcoh train 2.8041977882385254 and valid 1.1185276508331299\n",
      "after 810 epcoh train 2.8041844367980957 and valid 1.1185275316238403\n",
      "after 811 epcoh train 2.8041865825653076 and valid 1.1185914278030396\n",
      "after 812 epcoh train 2.804187297821045 and valid 1.1185266971588135\n",
      "after 813 epcoh train 2.8041903972625732 and valid 1.1185283660888672\n",
      "after 814 epcoh train 2.8042266368865967 and valid 1.1185262203216553\n",
      "after 815 epcoh train 2.804184675216675 and valid 1.1185269355773926\n",
      "after 816 epcoh train 2.8041865825653076 and valid 1.352918267250061\n",
      "after 817 epcoh train 2.8042027950286865 and valid 1.1185302734375\n",
      "after 818 epcoh train 2.8042149543762207 and valid 1.1186003684997559\n",
      "after 819 epcoh train 2.8042147159576416 and valid 1.118526816368103\n",
      "after 820 epcoh train 2.8042163848876953 and valid 1.118571400642395\n",
      "after 821 epcoh train 2.8041844367980957 and valid 1.1185925006866455\n",
      "after 822 epcoh train 2.8041958808898926 and valid 1.118553638458252\n",
      "after 823 epcoh train 2.8042025566101074 and valid 1.1185252666473389\n",
      "after 824 epcoh train 2.8718931674957275 and valid 1.1185251474380493\n",
      "after 825 epcoh train 2.8042123317718506 and valid 1.118532419204712\n",
      "after 826 epcoh train 2.8041863441467285 and valid 1.1185252666473389\n",
      "after 827 epcoh train 2.804211378097534 and valid 1.1185336112976074\n",
      "after 828 epcoh train 2.8719394207000732 and valid 1.1185431480407715\n",
      "after 829 epcoh train 2.8041834831237793 and valid 1.1185674667358398\n",
      "after 830 epcoh train 2.8041832447052 and valid 1.118539810180664\n",
      "after 831 epcoh train 2.804187297821045 and valid 1.1185250282287598\n",
      "after 832 epcoh train 2.804203510284424 and valid 1.1185258626937866\n",
      "after 833 epcoh train 2.8041844367980957 and valid 1.3529000282287598\n",
      "after 834 epcoh train 2.8041841983795166 and valid 1.1185262203216553\n",
      "after 835 epcoh train 2.871875524520874 and valid 1.1185269355773926\n",
      "after 836 epcoh train 2.8718924522399902 and valid 1.1185259819030762\n",
      "after 837 epcoh train 2.8041837215423584 and valid 1.1185252666473389\n",
      "after 838 epcoh train 2.8042168617248535 and valid 1.1185253858566284\n",
      "after 839 epcoh train 2.8719077110290527 and valid 1.118526577949524\n",
      "after 840 epcoh train 2.804222345352173 and valid 1.1185412406921387\n",
      "after 841 epcoh train 2.8041906356811523 and valid 1.118525505065918\n",
      "after 842 epcoh train 2.8041818141937256 and valid 1.352900505065918\n",
      "after 843 epcoh train 2.804201126098633 and valid 1.118525743484497\n",
      "after 844 epcoh train 2.8718924522399902 and valid 1.118525743484497\n",
      "after 845 epcoh train 2.804187059402466 and valid 1.1185266971588135\n",
      "after 846 epcoh train 2.8042030334472656 and valid 1.1185262203216553\n",
      "after 847 epcoh train 2.804180145263672 and valid 1.1185262203216553\n",
      "after 848 epcoh train 2.8042075634002686 and valid 1.1185452938079834\n",
      "after 849 epcoh train 2.804185390472412 and valid 1.1185336112976074\n",
      "after 850 epcoh train 2.8041951656341553 and valid 1.1185461282730103\n",
      "after 851 epcoh train 2.8041837215423584 and valid 1.118525743484497\n",
      "after 852 epcoh train 2.8042099475860596 and valid 1.352900743484497\n",
      "after 853 epcoh train 2.8042237758636475 and valid 1.1185433864593506\n",
      "after 854 epcoh train 2.8719122409820557 and valid 1.1185266971588135\n",
      "after 855 epcoh train 2.8718910217285156 and valid 1.1185259819030762\n",
      "after 856 epcoh train 2.8041794300079346 and valid 1.1185259819030762\n",
      "after 857 epcoh train 2.804187774658203 and valid 1.1185280084609985\n",
      "after 858 epcoh train 2.871889352798462 and valid 1.1185269355773926\n",
      "after 859 epcoh train 2.8042232990264893 and valid 1.1186184883117676\n",
      "after 860 epcoh train 2.804182291030884 and valid 1.1185261011123657\n",
      "after 861 epcoh train 2.804182291030884 and valid 1.3529009819030762\n",
      "after 862 epcoh train 2.8042190074920654 and valid 1.1185269355773926\n",
      "after 863 epcoh train 2.8041977882385254 and valid 1.1185269355773926\n",
      "after 864 epcoh train 2.8041913509368896 and valid 1.1185674667358398\n",
      "after 865 epcoh train 2.804182529449463 and valid 1.1185259819030762\n",
      "after 866 epcoh train 2.80417799949646 and valid 1.1185269355773926\n",
      "after 867 epcoh train 2.8041884899139404 and valid 1.1185259819030762\n",
      "after 868 epcoh train 2.80417799949646 and valid 1.1185261011123657\n",
      "after 869 epcoh train 2.871889352798462 and valid 1.1185261011123657\n",
      "after 870 epcoh train 2.8041927814483643 and valid 1.1185259819030762\n",
      "after 871 epcoh train 2.804201602935791 and valid 1.1185263395309448\n",
      "after 872 epcoh train 2.8042051792144775 and valid 1.1185276508331299\n",
      "after 873 epcoh train 2.804196357727051 and valid 1.3529016971588135\n",
      "after 874 epcoh train 2.8041977882385254 and valid 1.1185472011566162\n",
      "after 875 epcoh train 2.8041903972625732 and valid 1.1185271739959717\n",
      "after 876 epcoh train 2.8041791915893555 and valid 1.1185270547866821\n",
      "after 877 epcoh train 2.804184913635254 and valid 1.118527889251709\n",
      "after 878 epcoh train 2.8041884899139404 and valid 1.118527889251709\n",
      "after 879 epcoh train 2.804211378097534 and valid 1.1185270547866821\n",
      "after 880 epcoh train 2.8041765689849854 and valid 1.3529030084609985\n",
      "after 881 epcoh train 2.804203987121582 and valid 1.1185269355773926\n",
      "after 882 epcoh train 2.804176092147827 and valid 1.118527889251709\n",
      "after 883 epcoh train 2.8719029426574707 and valid 1.1185269355773926\n",
      "after 884 epcoh train 2.8041775226593018 and valid 1.118597149848938\n",
      "after 885 epcoh train 2.8041927814483643 and valid 1.1185266971588135\n",
      "after 886 epcoh train 2.8041768074035645 and valid 1.118588924407959\n",
      "after 887 epcoh train 2.804203987121582 and valid 1.1185258626937866\n",
      "after 888 epcoh train 2.871899366378784 and valid 1.1185709238052368\n",
      "after 889 epcoh train 2.8042054176330566 and valid 1.3529000282287598\n",
      "after 890 epcoh train 2.8042025566101074 and valid 1.1185249090194702\n",
      "after 891 epcoh train 2.8042004108428955 and valid 1.118539810180664\n",
      "after 892 epcoh train 2.804177761077881 and valid 1.118525743484497\n",
      "after 893 epcoh train 2.8041982650756836 and valid 1.1185276508331299\n",
      "after 894 epcoh train 2.804180860519409 and valid 1.1185258626937866\n",
      "after 895 epcoh train 2.804236650466919 and valid 1.1185259819030762\n",
      "after 896 epcoh train 2.8041841983795166 and valid 1.1185259819030762\n",
      "after 897 epcoh train 2.8041763305664062 and valid 1.1185295581817627\n",
      "after 898 epcoh train 2.804189443588257 and valid 1.11863112449646\n",
      "after 899 epcoh train 2.8041763305664062 and valid 1.1185249090194702\n",
      "after 900 epcoh train 2.871891736984253 and valid 1.1185252666473389\n",
      "after 901 epcoh train 2.8041880130767822 and valid 1.3529322147369385\n",
      "after 902 epcoh train 2.80417799949646 and valid 1.1185243129730225\n",
      "after 903 epcoh train 2.8042049407958984 and valid 1.118537425994873\n",
      "after 904 epcoh train 2.8041772842407227 and valid 1.1185243129730225\n",
      "after 905 epcoh train 2.8041770458221436 and valid 1.1185848712921143\n",
      "after 906 epcoh train 2.8041775226593018 and valid 1.1185383796691895\n",
      "after 907 epcoh train 2.8718996047973633 and valid 1.3528997898101807\n",
      "after 908 epcoh train 2.8042209148406982 and valid 1.1185240745544434\n",
      "after 909 epcoh train 2.804180383682251 and valid 1.118552327156067\n",
      "after 910 epcoh train 2.8042056560516357 and valid 1.1185250282287598\n",
      "after 911 epcoh train 2.8042125701904297 and valid 1.1185245513916016\n",
      "after 912 epcoh train 2.8041751384735107 and valid 1.1185247898101807\n",
      "after 913 epcoh train 2.804178237915039 and valid 1.1185247898101807\n",
      "after 914 epcoh train 2.804202079772949 and valid 1.1185405254364014\n",
      "after 915 epcoh train 2.804175615310669 and valid 1.1185246706008911\n",
      "after 916 epcoh train 2.8041911125183105 and valid 1.1185400485992432\n",
      "after 917 epcoh train 2.8041954040527344 and valid 1.1185252666473389\n",
      "after 918 epcoh train 2.804185152053833 and valid 1.1185246706008911\n",
      "after 919 epcoh train 2.8041741847991943 and valid 1.1185252666473389\n",
      "after 920 epcoh train 2.8041765689849854 and valid 1.1185247898101807\n",
      "after 921 epcoh train 2.8041749000549316 and valid 1.1185247898101807\n",
      "after 922 epcoh train 2.804183006286621 and valid 1.118525505065918\n",
      "after 923 epcoh train 2.8041770458221436 and valid 1.1185249090194702\n",
      "after 924 epcoh train 2.804173469543457 and valid 1.118532657623291\n",
      "after 925 epcoh train 2.804173231124878 and valid 1.1185259819030762\n",
      "after 926 epcoh train 2.8041832447052 and valid 1.1185249090194702\n",
      "after 927 epcoh train 2.8041775226593018 and valid 1.3529009819030762\n",
      "after 928 epcoh train 2.8042006492614746 and valid 1.1185250282287598\n",
      "after 929 epcoh train 2.8041768074035645 and valid 1.1185251474380493\n",
      "after 930 epcoh train 2.8042263984680176 and valid 1.1185259819030762\n",
      "after 931 epcoh train 2.8041722774505615 and valid 1.1185572147369385\n",
      "after 932 epcoh train 2.8041985034942627 and valid 1.1185262203216553\n",
      "after 933 epcoh train 2.8041763305664062 and valid 1.1185433864593506\n",
      "after 934 epcoh train 2.8718817234039307 and valid 1.118525505065918\n",
      "after 935 epcoh train 2.804187297821045 and valid 1.1185688972473145\n",
      "after 936 epcoh train 2.8042140007019043 and valid 1.1185258626937866\n",
      "after 937 epcoh train 2.8041727542877197 and valid 1.1185251474380493\n",
      "after 938 epcoh train 2.8041720390319824 and valid 1.1185922622680664\n",
      "after 939 epcoh train 2.8042006492614746 and valid 1.118525743484497\n",
      "after 940 epcoh train 2.804213762283325 and valid 1.1185247898101807\n",
      "after 941 epcoh train 2.8041799068450928 and valid 1.1185848712921143\n",
      "after 942 epcoh train 2.80420184135437 and valid 1.1185660362243652\n",
      "after 943 epcoh train 2.804178237915039 and valid 1.1185240745544434\n",
      "after 944 epcoh train 2.8041739463806152 and valid 1.1185381412506104\n",
      "after 945 epcoh train 2.8718912601470947 and valid 1.1185237169265747\n",
      "after 946 epcoh train 2.8041915893554688 and valid 1.1185237169265747\n",
      "after 947 epcoh train 2.8041961193084717 and valid 1.118524193763733\n",
      "after 948 epcoh train 2.8042049407958984 and valid 1.1185249090194702\n",
      "after 949 epcoh train 2.8041746616363525 and valid 1.1185760498046875\n",
      "after 950 epcoh train 2.804173469543457 and valid 1.1185247898101807\n",
      "after 951 epcoh train 2.8041954040527344 and valid 1.1185237169265747\n",
      "after 952 epcoh train 2.80417537689209 and valid 1.1185712814331055\n",
      "after 953 epcoh train 2.804250478744507 and valid 1.1185238361358643\n",
      "after 954 epcoh train 2.871880054473877 and valid 1.1185253858566284\n",
      "after 955 epcoh train 2.804173469543457 and valid 1.1185238361358643\n",
      "after 956 epcoh train 2.8041985034942627 and valid 1.1185250282287598\n",
      "after 957 epcoh train 2.871910333633423 and valid 1.1185243129730225\n",
      "after 958 epcoh train 2.8042073249816895 and valid 1.1185258626937866\n",
      "after 959 epcoh train 2.804175853729248 and valid 1.3529062271118164\n",
      "after 960 epcoh train 2.8041810989379883 and valid 1.118542194366455\n",
      "after 961 epcoh train 2.8041698932647705 and valid 1.1185246706008911\n",
      "after 962 epcoh train 2.8041911125183105 and valid 1.1185247898101807\n",
      "after 963 epcoh train 2.80417799949646 and valid 1.3528995513916016\n",
      "after 964 epcoh train 2.8041791915893555 and valid 1.118525505065918\n",
      "after 965 epcoh train 2.804196834564209 and valid 1.1185472011566162\n",
      "after 966 epcoh train 2.8041694164276123 and valid 1.1185322999954224\n",
      "after 967 epcoh train 2.804168701171875 and valid 1.1185250282287598\n",
      "after 968 epcoh train 2.8718559741973877 and valid 1.118546485900879\n",
      "after 969 epcoh train 2.804197311401367 and valid 1.118525743484497\n",
      "after 970 epcoh train 2.804170608520508 and valid 1.1185801029205322\n",
      "after 971 epcoh train 2.804168462753296 and valid 1.1185438632965088\n",
      "after 972 epcoh train 2.804175853729248 and valid 1.1185247898101807\n",
      "after 973 epcoh train 2.804202079772949 and valid 1.1185429096221924\n",
      "after 974 epcoh train 2.8718860149383545 and valid 1.1185461282730103\n",
      "after 975 epcoh train 2.804206371307373 and valid 1.1185246706008911\n",
      "after 976 epcoh train 2.8041796684265137 and valid 1.118526577949524\n",
      "after 977 epcoh train 2.871856212615967 and valid 1.1185252666473389\n",
      "after 978 epcoh train 2.8042025566101074 and valid 1.1185429096221924\n",
      "after 979 epcoh train 2.8042314052581787 and valid 1.1185246706008911\n",
      "after 980 epcoh train 2.93959903717041 and valid 1.118546485900879\n",
      "after 981 epcoh train 2.804173231124878 and valid 1.1185247898101807\n",
      "after 982 epcoh train 2.804175853729248 and valid 1.352900743484497\n",
      "after 983 epcoh train 2.804180860519409 and valid 1.118525743484497\n",
      "after 984 epcoh train 2.8041739463806152 and valid 1.1185287237167358\n",
      "after 985 epcoh train 2.804166555404663 and valid 1.118532419204712\n",
      "after 986 epcoh train 2.8042070865631104 and valid 1.1185436248779297\n",
      "after 987 epcoh train 2.8041858673095703 and valid 1.1185250282287598\n",
      "after 988 epcoh train 2.8719098567962646 and valid 1.1185250282287598\n",
      "after 989 epcoh train 2.804180383682251 and valid 1.1185321807861328\n",
      "after 990 epcoh train 2.80419921875 and valid 1.1185250282287598\n",
      "after 991 epcoh train 2.8041980266571045 and valid 1.1185252666473389\n",
      "after 992 epcoh train 2.8041727542877197 and valid 1.1185461282730103\n",
      "after 993 epcoh train 2.804168939590454 and valid 1.1185252666473389\n",
      "after 994 epcoh train 2.87188982963562 and valid 1.1185269355773926\n",
      "after 995 epcoh train 2.8041882514953613 and valid 1.3529623746871948\n",
      "after 996 epcoh train 2.8041722774505615 and valid 1.118525505065918\n",
      "after 997 epcoh train 2.8041985034942627 and valid 1.118525505065918\n",
      "after 998 epcoh train 2.804168701171875 and valid 1.118525505065918\n",
      "after 999 epcoh train 2.8041656017303467 and valid 1.118525505065918\n"
     ]
    }
   ],
   "source": [
    "model=CNN_model()\n",
    "model.to(device='cuda:0')\n",
    "LF=nn.CrossEntropyLoss()\n",
    "LF.to(device='cuda:0')\n",
    "optim=torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "prev_valid=np.inf\n",
    "for epoch in range(1000):\n",
    "    train=0\n",
    "    model.train()\n",
    "    for fea,label in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        y=model(fea.unsqueeze(dim=1).to(device='cuda:0'))\n",
    "        loss=LF(y,label.to(device='cuda:0'))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            train+=loss\n",
    "    valid=0\n",
    "    model.eval()\n",
    "    for fea,label in valid_dataloader:\n",
    "        model.zero_grad()\n",
    "        y=model(fea.unsqueeze(dim=1).to(device='cuda:0'))\n",
    "        label=label.to(device='cuda:0')\n",
    "        loss=LF(y,label)\n",
    "        # loss = Variable(loss, requires_grad = True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        with torch.no_grad():\n",
    "            valid+=loss\n",
    "    if valid<prev_valid:\n",
    "        prev_valid=valid\n",
    "        torch.save(model,\"CNN_vowel_mfcc\")\n",
    "    print(\"after {} epcoh train {} and valid {}\".format(epoch,train,valid))\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp=open('D:/codes/M.Tech_proj/test_clean_mfcc_cons','rb')\n",
    "dataset=pickle.load(fp)\n",
    "fp.close()\n",
    "test_data=DataLoader(dataset,batch_size=len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_model(\n",
       "  (lay1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lay2): ReLU()\n",
       "  (lay3): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (lay4): ReLU()\n",
       "  (lay6): Linear(in_features=1740, out_features=100, bias=True)\n",
       "  (lay7): Linear(in_features=100, out_features=3, bias=True)\n",
       "  (lay8): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fea,label in test_data:\n",
    "    y=model(fea.unsqueeze(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(y.detach().numpy().argmax(axis=1),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9880952380952381"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
